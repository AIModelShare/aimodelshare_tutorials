{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\"><img width=\"50%\" src=\"https://aimodelsharecontent.s3.amazonaws.com/aimodshare_banner.jpg\" /></p>\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "m_DMeZsXxpEZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXxGTgJz152A"
      },
      "source": [
        "<p align=\"center\"><h1 align=\"center\">Flower Image Classification Model Submission Guide - Predictions Only (No Model Metadata Extraction)\n",
        "\n",
        "---\n",
        "Let's share our models to a centralized leaderboard, so that we can collaborate and learn from the model experimentation process...\n",
        "\n",
        "**Instructions:**\n",
        "1.   Get data in and set up X_train / X_test / y_train\n",
        "2.   Preprocess data\n",
        "3. Fit model on preprocessed data\n",
        "4. Generate predictions from X_test data and submit to competition\n",
        "5. Repeat submission process to improve place on leaderboard\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gSrVJwp3E9H"
      },
      "source": [
        "## 1. Get data in and set up X_train, X_test, y_train objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLTIaMB3ChSW"
      },
      "source": [
        "#install aimodelshare library\n",
        "! pip install aimodelshare --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3PiJXBhC5y-",
        "outputId": "53003875-0d29-4ba3-efb7-656d9a32b724"
      },
      "source": [
        "# Get competition data\n",
        "from aimodelshare import download_data\n",
        "download_data('public.ecr.aws/y2e2a1d6/flower_competition_data-repository:latest') "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading [================================================>]\n",
            "\n",
            "Data downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training data objects \n",
        "\n",
        "# Extracting all filepaths iteratively...\n",
        "import os \n",
        "\n",
        "base_path = 'flower_competition_data/train_images'\n",
        "categories = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
        "\n",
        "# Load file paths to fnames list object...\n",
        "fnames = []\n",
        "for category in categories:\n",
        "    flower_folder = os.path.join(base_path, category)\n",
        "    file_names = os.listdir(flower_folder)\n",
        "    full_path = [os.path.join(flower_folder, file_name) for file_name in file_names]\n",
        "    fnames.append(full_path)\n",
        "\n",
        "print('number of images for each category:', [len(f) for f in fnames])\n",
        "print(fnames[0][1]) # Examples of file names...\n",
        "\n",
        "# Create y data made up of correctly ordered labels from file folders...\n",
        "from itertools import repeat\n",
        "\n",
        "daisy = list(repeat(\"daisy\", 507)) #i.e.: 507 filenames in daisy folder\n",
        "dandelion = list(repeat(\"dandelion\", 718))\n",
        "roses = list(repeat(\"roses\", 513))\n",
        "sunflowers = list(repeat(\"sunflowers\", 559))\n",
        "tulips = list(repeat(\"tulips\", 639))\n",
        "\n",
        "# Combine into single list of y labels...\n",
        "y_labels = daisy + dandelion + roses + sunflowers + tulips \n",
        "\n",
        "# Need to one-hot encode for Keras. Let's use Pandas...\n",
        "import pandas as pd\n",
        "y_train = pd.get_dummies(y_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KujkkcWsiGFO",
        "outputId": "56e03e16-5ba6-41eb-cb63-a8a1951c3c35"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of images for each category: [507, 718, 513, 559, 639]\n",
            "flower_competition_data/train_images/daisy/8882282142_9be2524d38_m.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEzPoXPj3V7u"
      },
      "source": [
        "##2.   Preprocess data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write and execute code to preprocess data here - an example is provided below "
      ],
      "metadata": {
        "id": "6IKOmKsXiZjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is a pre-designed preprocessor, but you could also build your own to prepare the data differently\n",
        "\n",
        "def preprocessor(image_filepath, shape=(192, 192)):\n",
        "        \"\"\"\n",
        "        This function preprocesses reads in images, resizes them to a fixed shape and\n",
        "        min/max transforms them before converting feature values to float32 numeric values\n",
        "        required by onnx files.\n",
        "        \n",
        "        params:\n",
        "            image_filepath\n",
        "                full filepath of a particular image\n",
        "                      \n",
        "        returns:\n",
        "            X\n",
        "                numpy array of preprocessed image data\n",
        "                  \n",
        "        \"\"\"\n",
        "           \n",
        "        import cv2\n",
        "        import numpy as np\n",
        "\n",
        "        \"Resize a color image and min/max transform the image\"\n",
        "        img = cv2.imread(image_filepath) # Read in image from filepath.\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # cv2 reads in images in order of blue green and red, we reverse the order for ML.\n",
        "        img = cv2.resize(img, shape) # Change height and width of image.\n",
        "        img = img / 255.0 # Min-max transform.\n",
        "\n",
        "\n",
        "        # Resize all the images...\n",
        "        X = np.array(img)\n",
        "        X = np.expand_dims(X, axis=0) # Expand dims to add \"1\" to object shape [1, h, w, channels] for keras model.\n",
        "        X = np.array(X, dtype=np.float32) # Final shape for onnx runtime.\n",
        "        return X"
      ],
      "metadata": {
        "id": "wgZFbZjKjzoH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use preprocessor to create X_train object \n",
        "\n",
        "# Import image, load to array of shape height, width, channels, then min/max transform...\n",
        "# Read in all images from filenames...\n",
        "import numpy as np \n",
        "\n",
        "preprocessed_image_data = [preprocessor(x) for x in fnames[0] + fnames[1] + fnames[2] + \n",
        "                           fnames[3] + fnames[4]]\n",
        "\n",
        "# Object needs to be an array rather than a list for Keras. (vstack converts above list to array object.)\n",
        "X_train = np.vstack(preprocessed_image_data)\n",
        "# Assigning to X_train to highlight that this represents feature input data for our model."
      ],
      "metadata": {
        "id": "Rmk3I_ysj0UE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess X_test image data to generate predictions from models \n",
        "import numpy as np\n",
        "\n",
        "file_names = [('flower_competition_data/test_images/' + str(i) + '.jpg') for i in range(1, 735)]\n",
        "\n",
        "preprocessed_image_data = [preprocessor(x) for x in file_names]\n",
        "\n",
        "#Create single X_test array from preprocessed images\n",
        "X_test = np.vstack(preprocessed_image_data) "
      ],
      "metadata": {
        "id": "G_sdpBG9j3YS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X52kECL43b-O"
      },
      "source": [
        "##3. Fit model on preprocessed data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCbBf8j9ClYl"
      },
      "source": [
        "# Write and execute code to fit model on preprocessed data here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtgkM02MDpkO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec7a126f-ddfc-442a-ad7e-97d5726c3518"
      },
      "source": [
        "#Set credentials using modelshare.org username/password\n",
        "\n",
        "from aimodelshare.aws import set_credentials\n",
        "    \n",
        "apiurl=\"https://kbigfdp8xf.execute-api.us-east-1.amazonaws.com/prod/m\"\n",
        "#This is the unique rest api that powers this Fashhion-MNIST Image Classification Playground\n",
        "\n",
        "set_credentials(apiurl=apiurl)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Modelshare Username:··········\n",
            "AI Modelshare Password:··········\n",
            "AI Model Share login credentials set successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKNGSww8EGgi"
      },
      "source": [
        "#Instantiate Competition\n",
        "import aimodelshare as ai\n",
        "mycompetition= ai.Competition(apiurl)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Submit Model predictions to leaderboard (without extracting model architecture information):**\n",
        "- model metadata extraction allows you use compare_models() and instantiate_model() functions."
      ],
      "metadata": {
        "id": "W66Pr1_K2sQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate a list of predictions using X_test data\n",
        "\n",
        "# This example uses randomly chosen values from y_labels to generate a list of predictions\n",
        "import random \n",
        "predicted_values = random.sample(y_labels, len(X_test))"
      ],
      "metadata": {
        "id": "IS09g9DF0HGP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ql4wksyEUnP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37af1b61-2dc5-4239-8c6a-77a19eaef13a"
      },
      "source": [
        "#Submit Model predictions to leaderboard (without extracting model architecture information): \n",
        "\n",
        "# Submit to Competition Leaderboard\n",
        "mycompetition.submit_model(model_filepath = None,\n",
        "                                 preprocessor_filepath=None,\n",
        "                                 prediction_submission=predicted_values)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Insert search tags to help users find your model (optional): \n",
            "Provide any useful notes about your model (optional): \n",
            "\n",
            "Your model has been submitted as model version 6\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "GN1zvAmNEq17",
        "outputId": "e88e50a0-0d1f-4b20-c492-b2be036bc324"
      },
      "source": [
        "# Get leaderboard to explore current best model architectures\n",
        "\n",
        "# Get raw data in pandas data frame\n",
        "data = mycompetition.get_leaderboard()\n",
        "\n",
        "# Stylize leaderboard data\n",
        "mycompetition.stylize_leaderboard(data)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fb6f303df50>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_d363f_row0_col0 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#f5f8d6 61.6%, transparent 61.6%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d363f_row0_col1 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#c778c8 60.9%, transparent 60.9%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d363f_row0_col2 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#ff4971 61.9%, transparent 61.9%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d363f_row0_col3 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#aadbaa 61.3%, transparent 61.3%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d363f_row0_col4, #T_d363f_row0_col5, #T_d363f_row0_col6, #T_d363f_row0_col7, #T_d363f_row0_col8, #T_d363f_row0_col9, #T_d363f_row0_col10, #T_d363f_row0_col11, #T_d363f_row0_col12, #T_d363f_row0_col13, #T_d363f_row0_col14, #T_d363f_row0_col15, #T_d363f_row0_col16, #T_d363f_row0_col17, #T_d363f_row0_col18, #T_d363f_row0_col19, #T_d363f_row0_col20, #T_d363f_row1_col4, #T_d363f_row1_col5, #T_d363f_row1_col6, #T_d363f_row1_col7, #T_d363f_row1_col8, #T_d363f_row1_col9, #T_d363f_row1_col10, #T_d363f_row1_col11, #T_d363f_row1_col12, #T_d363f_row1_col13, #T_d363f_row1_col14, #T_d363f_row1_col15, #T_d363f_row1_col16, #T_d363f_row1_col17, #T_d363f_row1_col18, #T_d363f_row1_col19, #T_d363f_row1_col20, #T_d363f_row2_col4, #T_d363f_row2_col5, #T_d363f_row2_col6, #T_d363f_row2_col7, #T_d363f_row2_col8, #T_d363f_row2_col9, #T_d363f_row2_col10, #T_d363f_row2_col11, #T_d363f_row2_col12, #T_d363f_row2_col13, #T_d363f_row2_col14, #T_d363f_row2_col15, #T_d363f_row2_col16, #T_d363f_row2_col17, #T_d363f_row2_col18, #T_d363f_row2_col19, #T_d363f_row2_col20, #T_d363f_row3_col4, #T_d363f_row3_col5, #T_d363f_row3_col6, #T_d363f_row3_col7, #T_d363f_row3_col8, #T_d363f_row3_col9, #T_d363f_row3_col10, #T_d363f_row3_col11, #T_d363f_row3_col12, #T_d363f_row3_col13, #T_d363f_row3_col14, #T_d363f_row3_col15, #T_d363f_row3_col16, #T_d363f_row3_col17, #T_d363f_row3_col18, #T_d363f_row3_col19, #T_d363f_row3_col20, #T_d363f_row4_col4, #T_d363f_row4_col5, #T_d363f_row4_col6, #T_d363f_row4_col7, #T_d363f_row4_col8, #T_d363f_row4_col9, #T_d363f_row4_col10, #T_d363f_row4_col11, #T_d363f_row4_col12, #T_d363f_row4_col13, #T_d363f_row4_col14, #T_d363f_row4_col15, #T_d363f_row4_col16, #T_d363f_row4_col17, #T_d363f_row4_col18, #T_d363f_row4_col19, #T_d363f_row4_col20, #T_d363f_row5_col4, #T_d363f_row5_col5, #T_d363f_row5_col6, #T_d363f_row5_col7, #T_d363f_row5_col8, #T_d363f_row5_col9, #T_d363f_row5_col10, #T_d363f_row5_col11, #T_d363f_row5_col12, #T_d363f_row5_col13, #T_d363f_row5_col14, #T_d363f_row5_col15, #T_d363f_row5_col16, #T_d363f_row5_col17, #T_d363f_row5_col18, #T_d363f_row5_col19, #T_d363f_row5_col20 {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_d363f_row1_col0 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#f5f8d6 46.3%, transparent 46.3%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d363f_row1_col1 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#c778c8 45.2%, transparent 45.2%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d363f_row1_col2 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#ff4971 48.0%, transparent 48.0%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d363f_row1_col3 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#aadbaa 44.7%, transparent 44.7%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d363f_row2_col0 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#f5f8d6 39.0%, transparent 39.0%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d363f_row2_col1 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#c778c8 33.4%, transparent 33.4%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d363f_row2_col2 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#ff4971 32.2%, transparent 32.2%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d363f_row2_col3 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#aadbaa 40.9%, transparent 40.9%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d363f_row3_col0 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#f5f8d6 37.6%, transparent 37.6%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d363f_row3_col1 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#c778c8 27.4%, transparent 27.4%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d363f_row3_col2 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#ff4971 38.3%, transparent 38.3%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d363f_row3_col3 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#aadbaa 35.0%, transparent 35.0%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d363f_row4_col0 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#f5f8d6 32.2%, transparent 32.2%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d363f_row4_col1 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#c778c8 21.9%, transparent 21.9%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d363f_row4_col2 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#ff4971 34.0%, transparent 34.0%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d363f_row4_col3 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#aadbaa 28.0%, transparent 28.0%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d363f_row5_col0 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#f5f8d6 22.3%, transparent 22.3%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d363f_row5_col1 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#c778c8 21.8%, transparent 21.8%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d363f_row5_col2 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#ff4971 21.9%, transparent 21.9%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d363f_row5_col3 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#aadbaa 21.8%, transparent 21.8%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_d363f_\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" >accuracy</th>\n",
              "      <th class=\"col_heading level0 col1\" >f1_score</th>\n",
              "      <th class=\"col_heading level0 col2\" >precision</th>\n",
              "      <th class=\"col_heading level0 col3\" >recall</th>\n",
              "      <th class=\"col_heading level0 col4\" >ml_framework</th>\n",
              "      <th class=\"col_heading level0 col5\" >deep_learning</th>\n",
              "      <th class=\"col_heading level0 col6\" >model_type</th>\n",
              "      <th class=\"col_heading level0 col7\" >depth</th>\n",
              "      <th class=\"col_heading level0 col8\" >num_params</th>\n",
              "      <th class=\"col_heading level0 col9\" >maxpooling2d_layers</th>\n",
              "      <th class=\"col_heading level0 col10\" >conv2d_layers</th>\n",
              "      <th class=\"col_heading level0 col11\" >dense_layers</th>\n",
              "      <th class=\"col_heading level0 col12\" >dropout_layers</th>\n",
              "      <th class=\"col_heading level0 col13\" >flatten_layers</th>\n",
              "      <th class=\"col_heading level0 col14\" >softmax_act</th>\n",
              "      <th class=\"col_heading level0 col15\" >relu_act</th>\n",
              "      <th class=\"col_heading level0 col16\" >loss</th>\n",
              "      <th class=\"col_heading level0 col17\" >optimizer</th>\n",
              "      <th class=\"col_heading level0 col18\" >memory_size</th>\n",
              "      <th class=\"col_heading level0 col19\" >username</th>\n",
              "      <th class=\"col_heading level0 col20\" >version</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_d363f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_d363f_row0_col0\" class=\"data row0 col0\" >61.58%</td>\n",
              "      <td id=\"T_d363f_row0_col1\" class=\"data row0 col1\" >60.94%</td>\n",
              "      <td id=\"T_d363f_row0_col2\" class=\"data row0 col2\" >61.89%</td>\n",
              "      <td id=\"T_d363f_row0_col3\" class=\"data row0 col3\" >61.32%</td>\n",
              "      <td id=\"T_d363f_row0_col4\" class=\"data row0 col4\" >keras</td>\n",
              "      <td id=\"T_d363f_row0_col5\" class=\"data row0 col5\" >True</td>\n",
              "      <td id=\"T_d363f_row0_col6\" class=\"data row0 col6\" >Sequential</td>\n",
              "      <td id=\"T_d363f_row0_col7\" class=\"data row0 col7\" >11.000000</td>\n",
              "      <td id=\"T_d363f_row0_col8\" class=\"data row0 col8\" >1851153.000000</td>\n",
              "      <td id=\"T_d363f_row0_col9\" class=\"data row0 col9\" >2.000000</td>\n",
              "      <td id=\"T_d363f_row0_col10\" class=\"data row0 col10\" >4.000000</td>\n",
              "      <td id=\"T_d363f_row0_col11\" class=\"data row0 col11\" >2.000000</td>\n",
              "      <td id=\"T_d363f_row0_col12\" class=\"data row0 col12\" >2.000000</td>\n",
              "      <td id=\"T_d363f_row0_col13\" class=\"data row0 col13\" >1.000000</td>\n",
              "      <td id=\"T_d363f_row0_col14\" class=\"data row0 col14\" >1.000000</td>\n",
              "      <td id=\"T_d363f_row0_col15\" class=\"data row0 col15\" >5.000000</td>\n",
              "      <td id=\"T_d363f_row0_col16\" class=\"data row0 col16\" >function</td>\n",
              "      <td id=\"T_d363f_row0_col17\" class=\"data row0 col17\" >RMSprop</td>\n",
              "      <td id=\"T_d363f_row0_col18\" class=\"data row0 col18\" >7406304.000000</td>\n",
              "      <td id=\"T_d363f_row0_col19\" class=\"data row0 col19\" >IEOR_ML_High_Dim</td>\n",
              "      <td id=\"T_d363f_row0_col20\" class=\"data row0 col20\" >2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d363f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_d363f_row1_col0\" class=\"data row1 col0\" >46.32%</td>\n",
              "      <td id=\"T_d363f_row1_col1\" class=\"data row1 col1\" >45.21%</td>\n",
              "      <td id=\"T_d363f_row1_col2\" class=\"data row1 col2\" >48.01%</td>\n",
              "      <td id=\"T_d363f_row1_col3\" class=\"data row1 col3\" >44.66%</td>\n",
              "      <td id=\"T_d363f_row1_col4\" class=\"data row1 col4\" >keras</td>\n",
              "      <td id=\"T_d363f_row1_col5\" class=\"data row1 col5\" >True</td>\n",
              "      <td id=\"T_d363f_row1_col6\" class=\"data row1 col6\" >Sequential</td>\n",
              "      <td id=\"T_d363f_row1_col7\" class=\"data row1 col7\" >5.000000</td>\n",
              "      <td id=\"T_d363f_row1_col8\" class=\"data row1 col8\" >11805061.000000</td>\n",
              "      <td id=\"T_d363f_row1_col9\" class=\"data row1 col9\" >nan</td>\n",
              "      <td id=\"T_d363f_row1_col10\" class=\"data row1 col10\" >nan</td>\n",
              "      <td id=\"T_d363f_row1_col11\" class=\"data row1 col11\" >4.000000</td>\n",
              "      <td id=\"T_d363f_row1_col12\" class=\"data row1 col12\" >nan</td>\n",
              "      <td id=\"T_d363f_row1_col13\" class=\"data row1 col13\" >1.000000</td>\n",
              "      <td id=\"T_d363f_row1_col14\" class=\"data row1 col14\" >1.000000</td>\n",
              "      <td id=\"T_d363f_row1_col15\" class=\"data row1 col15\" >3.000000</td>\n",
              "      <td id=\"T_d363f_row1_col16\" class=\"data row1 col16\" >function</td>\n",
              "      <td id=\"T_d363f_row1_col17\" class=\"data row1 col17\" >RMSprop</td>\n",
              "      <td id=\"T_d363f_row1_col18\" class=\"data row1 col18\" >47221296.000000</td>\n",
              "      <td id=\"T_d363f_row1_col19\" class=\"data row1 col19\" >IEOR_ML_High_Dim</td>\n",
              "      <td id=\"T_d363f_row1_col20\" class=\"data row1 col20\" >1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d363f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_d363f_row2_col0\" class=\"data row2 col0\" >38.96%</td>\n",
              "      <td id=\"T_d363f_row2_col1\" class=\"data row2 col1\" >33.41%</td>\n",
              "      <td id=\"T_d363f_row2_col2\" class=\"data row2 col2\" >32.20%</td>\n",
              "      <td id=\"T_d363f_row2_col3\" class=\"data row2 col3\" >40.88%</td>\n",
              "      <td id=\"T_d363f_row2_col4\" class=\"data row2 col4\" >keras</td>\n",
              "      <td id=\"T_d363f_row2_col5\" class=\"data row2 col5\" >True</td>\n",
              "      <td id=\"T_d363f_row2_col6\" class=\"data row2 col6\" >Sequential</td>\n",
              "      <td id=\"T_d363f_row2_col7\" class=\"data row2 col7\" >5.000000</td>\n",
              "      <td id=\"T_d363f_row2_col8\" class=\"data row2 col8\" >11805061.000000</td>\n",
              "      <td id=\"T_d363f_row2_col9\" class=\"data row2 col9\" >nan</td>\n",
              "      <td id=\"T_d363f_row2_col10\" class=\"data row2 col10\" >nan</td>\n",
              "      <td id=\"T_d363f_row2_col11\" class=\"data row2 col11\" >4.000000</td>\n",
              "      <td id=\"T_d363f_row2_col12\" class=\"data row2 col12\" >nan</td>\n",
              "      <td id=\"T_d363f_row2_col13\" class=\"data row2 col13\" >1.000000</td>\n",
              "      <td id=\"T_d363f_row2_col14\" class=\"data row2 col14\" >1.000000</td>\n",
              "      <td id=\"T_d363f_row2_col15\" class=\"data row2 col15\" >3.000000</td>\n",
              "      <td id=\"T_d363f_row2_col16\" class=\"data row2 col16\" >str</td>\n",
              "      <td id=\"T_d363f_row2_col17\" class=\"data row2 col17\" >RMSprop</td>\n",
              "      <td id=\"T_d363f_row2_col18\" class=\"data row2 col18\" >47221296.000000</td>\n",
              "      <td id=\"T_d363f_row2_col19\" class=\"data row2 col19\" >IEOR_ML_High_Dim</td>\n",
              "      <td id=\"T_d363f_row2_col20\" class=\"data row2 col20\" >4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d363f_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_d363f_row3_col0\" class=\"data row3 col0\" >37.60%</td>\n",
              "      <td id=\"T_d363f_row3_col1\" class=\"data row3 col1\" >27.39%</td>\n",
              "      <td id=\"T_d363f_row3_col2\" class=\"data row3 col2\" >38.34%</td>\n",
              "      <td id=\"T_d363f_row3_col3\" class=\"data row3 col3\" >35.04%</td>\n",
              "      <td id=\"T_d363f_row3_col4\" class=\"data row3 col4\" >keras</td>\n",
              "      <td id=\"T_d363f_row3_col5\" class=\"data row3 col5\" >True</td>\n",
              "      <td id=\"T_d363f_row3_col6\" class=\"data row3 col6\" >Sequential</td>\n",
              "      <td id=\"T_d363f_row3_col7\" class=\"data row3 col7\" >11.000000</td>\n",
              "      <td id=\"T_d363f_row3_col8\" class=\"data row3 col8\" >1850913.000000</td>\n",
              "      <td id=\"T_d363f_row3_col9\" class=\"data row3 col9\" >2.000000</td>\n",
              "      <td id=\"T_d363f_row3_col10\" class=\"data row3 col10\" >4.000000</td>\n",
              "      <td id=\"T_d363f_row3_col11\" class=\"data row3 col11\" >2.000000</td>\n",
              "      <td id=\"T_d363f_row3_col12\" class=\"data row3 col12\" >2.000000</td>\n",
              "      <td id=\"T_d363f_row3_col13\" class=\"data row3 col13\" >1.000000</td>\n",
              "      <td id=\"T_d363f_row3_col14\" class=\"data row3 col14\" >1.000000</td>\n",
              "      <td id=\"T_d363f_row3_col15\" class=\"data row3 col15\" >5.000000</td>\n",
              "      <td id=\"T_d363f_row3_col16\" class=\"data row3 col16\" >str</td>\n",
              "      <td id=\"T_d363f_row3_col17\" class=\"data row3 col17\" >RMSprop</td>\n",
              "      <td id=\"T_d363f_row3_col18\" class=\"data row3 col18\" >7405344.000000</td>\n",
              "      <td id=\"T_d363f_row3_col19\" class=\"data row3 col19\" >IEOR_ML_High_Dim</td>\n",
              "      <td id=\"T_d363f_row3_col20\" class=\"data row3 col20\" >3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d363f_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_d363f_row4_col0\" class=\"data row4 col0\" >32.15%</td>\n",
              "      <td id=\"T_d363f_row4_col1\" class=\"data row4 col1\" >21.90%</td>\n",
              "      <td id=\"T_d363f_row4_col2\" class=\"data row4 col2\" >33.96%</td>\n",
              "      <td id=\"T_d363f_row4_col3\" class=\"data row4 col3\" >27.99%</td>\n",
              "      <td id=\"T_d363f_row4_col4\" class=\"data row4 col4\" >keras</td>\n",
              "      <td id=\"T_d363f_row4_col5\" class=\"data row4 col5\" >True</td>\n",
              "      <td id=\"T_d363f_row4_col6\" class=\"data row4 col6\" >Sequential</td>\n",
              "      <td id=\"T_d363f_row4_col7\" class=\"data row4 col7\" >11.000000</td>\n",
              "      <td id=\"T_d363f_row4_col8\" class=\"data row4 col8\" >1850913.000000</td>\n",
              "      <td id=\"T_d363f_row4_col9\" class=\"data row4 col9\" >2.000000</td>\n",
              "      <td id=\"T_d363f_row4_col10\" class=\"data row4 col10\" >4.000000</td>\n",
              "      <td id=\"T_d363f_row4_col11\" class=\"data row4 col11\" >2.000000</td>\n",
              "      <td id=\"T_d363f_row4_col12\" class=\"data row4 col12\" >2.000000</td>\n",
              "      <td id=\"T_d363f_row4_col13\" class=\"data row4 col13\" >1.000000</td>\n",
              "      <td id=\"T_d363f_row4_col14\" class=\"data row4 col14\" >1.000000</td>\n",
              "      <td id=\"T_d363f_row4_col15\" class=\"data row4 col15\" >5.000000</td>\n",
              "      <td id=\"T_d363f_row4_col16\" class=\"data row4 col16\" >str</td>\n",
              "      <td id=\"T_d363f_row4_col17\" class=\"data row4 col17\" >RMSprop</td>\n",
              "      <td id=\"T_d363f_row4_col18\" class=\"data row4 col18\" >7405344.000000</td>\n",
              "      <td id=\"T_d363f_row4_col19\" class=\"data row4 col19\" >IEOR_ML_High_Dim</td>\n",
              "      <td id=\"T_d363f_row4_col20\" class=\"data row4 col20\" >5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d363f_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_d363f_row5_col0\" class=\"data row5 col0\" >22.34%</td>\n",
              "      <td id=\"T_d363f_row5_col1\" class=\"data row5 col1\" >21.84%</td>\n",
              "      <td id=\"T_d363f_row5_col2\" class=\"data row5 col2\" >21.88%</td>\n",
              "      <td id=\"T_d363f_row5_col3\" class=\"data row5 col3\" >21.83%</td>\n",
              "      <td id=\"T_d363f_row5_col4\" class=\"data row5 col4\" >unknown</td>\n",
              "      <td id=\"T_d363f_row5_col5\" class=\"data row5 col5\" >nan</td>\n",
              "      <td id=\"T_d363f_row5_col6\" class=\"data row5 col6\" >unknown</td>\n",
              "      <td id=\"T_d363f_row5_col7\" class=\"data row5 col7\" >nan</td>\n",
              "      <td id=\"T_d363f_row5_col8\" class=\"data row5 col8\" >nan</td>\n",
              "      <td id=\"T_d363f_row5_col9\" class=\"data row5 col9\" >nan</td>\n",
              "      <td id=\"T_d363f_row5_col10\" class=\"data row5 col10\" >nan</td>\n",
              "      <td id=\"T_d363f_row5_col11\" class=\"data row5 col11\" >nan</td>\n",
              "      <td id=\"T_d363f_row5_col12\" class=\"data row5 col12\" >nan</td>\n",
              "      <td id=\"T_d363f_row5_col13\" class=\"data row5 col13\" >nan</td>\n",
              "      <td id=\"T_d363f_row5_col14\" class=\"data row5 col14\" >nan</td>\n",
              "      <td id=\"T_d363f_row5_col15\" class=\"data row5 col15\" >nan</td>\n",
              "      <td id=\"T_d363f_row5_col16\" class=\"data row5 col16\" >nan</td>\n",
              "      <td id=\"T_d363f_row5_col17\" class=\"data row5 col17\" >nan</td>\n",
              "      <td id=\"T_d363f_row5_col18\" class=\"data row5 col18\" >nan</td>\n",
              "      <td id=\"T_d363f_row5_col19\" class=\"data row5 col19\" >IEOR_ML_High_Dim</td>\n",
              "      <td id=\"T_d363f_row5_col20\" class=\"data row5 col20\" >6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also compare two or more models for any models submitted to the leaderboard using example code for model metadata extraction (see code tab for this competition at www.modelshare.org for submission examples.)\n",
        "```\n",
        "data=mycompetition.compare_models([1,2], verbose=1)\n",
        "mycompetition.stylize_compare(data)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "OK9F-jPs38K5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwNKs0wP4r5s"
      },
      "source": [
        "#####  (Optional Extension) Submit Model With Custom Metadata: \n",
        "Can use to add team names or any other missing data you may wish to share on the leaderboard\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgSs5PAtPCZH"
      },
      "source": [
        "# Custom metadata can be added by passing a dict to the custom_metadata argument of the submit_model() method\n",
        "# This option can be used to fill in missing data points or add new columns to the leaderboard\n",
        "\n",
        "custom_meta = {'team': 'team one',\n",
        "               'model_type': 'your_model_type',\n",
        "               'new_column': 'new metadata'}\n",
        "\n",
        "mycompetition.submit_model(model_filepath = None,\n",
        "                                 preprocessor_filepath=None,\n",
        "                                 prediction_submission=predicted_values,\n",
        "                                 custom_metadata = custom_meta)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}