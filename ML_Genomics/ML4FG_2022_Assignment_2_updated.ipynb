{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML4FG_2021_Assignment_2_updated.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q4rk9UBaAge"
      },
      "source": [
        "# Assignment 2 preamble\n",
        "\n",
        "Please remember to change the name of your copy of the assignment to include your name (and optionally UNI). \n",
        "\n",
        "We're going to try having you submit the ipynb file (File -> Download .ipynb) rather than sharing through Google drive this time. \n",
        "\n",
        "Most of the tasks/questions for this assignment are at the end but there is one in the middle so don't miss it! \n",
        "\n",
        "# Setting up Colab\n",
        "\n",
        "1.   Make sure you're signed into Colab through your Columbia account. This will be important for accessing data. \n",
        "2.   Check you've requested a GPU (or TPU): go to Runtime -> Change runtime type -> Set \"Hardware Accelerator\". Colab will restart if needed. \n",
        "\n",
        "Mount your Google drive. This should give you a link to follow where you'll copy a key that you'll paste in here. You can view files in the sidebar on the left under \"Files\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzkqFNhaY5i7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RP1SwXLwue-"
      },
      "source": [
        "Load `torch`. Note the version on colab can lag behind the most current release. If you need something new you can install using e.g. !pip3 install torch==1.4.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fPmCgelY66c"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "assert(torch.cuda.is_available()) # if this fails go to Runtime -> Change runtime type -> Set \"Hardware Accelerator\"\n",
        "print(\"Torch version:\", torch.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAngJKdYbckH"
      },
      "source": [
        "## Utilities\n",
        "\n",
        "We'll be using a \"one hot\" encoding of DNA sequence as the input to our CNN. This can actually be a bottleneck as it runs on the CPU so we use Cython to speed things up.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41Ks2b8EbQX_"
      },
      "source": [
        "%load_ext Cython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrq6O_UoaNn9"
      },
      "source": [
        "%%cython\n",
        "\n",
        "import numpy as np\n",
        "np.get_include() # do we need this on colab? \n",
        "cimport cython\n",
        "cimport numpy as np\n",
        "\n",
        "cdef dict bases={ 'A':<int>0, 'C':<int>1, 'G':<int>2, 'T':<int>3 } \n",
        "\n",
        "@cython.boundscheck(False)\n",
        "def one_hot( str string ):\n",
        "    cdef np.ndarray[np.float32_t, ndim=2] res = np.zeros( (4,len(string)), dtype=np.float32 )\n",
        "    cdef int j\n",
        "    for j in range(len(string)):\n",
        "        if string[j] in bases: # bases can be 'N' signifying missing: this corresponds to all 0 in the encoding\n",
        "            res[ bases[ string[j] ], j ]=float(1.0)\n",
        "    return(res)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPGBFsPlbzih"
      },
      "source": [
        "# Pytorch basics\n",
        "\n",
        "`pytorch` (as opposed to e.g. Theano, Tensorflow 1) uses *eager execution*: this lets you write computations as python code that you can test and debug, and later \n",
        "1.   Backprop through (i.e. get gradients with respect to inputs) \n",
        "2.   Run on the GPU for (hopefully!) big speedups. \n",
        "\n",
        "Here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VwOGPiSiBhz"
      },
      "source": [
        "x_np = one_hot(\"CCGCGNGGNGGCAG\")\n",
        "x_tensor = torch.tensor(x_np)\n",
        "print(x_tensor)\n",
        "torch.sum(x_tensor, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QswqklEZq3ci"
      },
      "source": [
        "`torch` has equivalents for most `numpy` operations, for example here we got the row sums to check how often each base appeared in our sequence. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtw8uxvUlSnx"
      },
      "source": [
        "As a more exciting example, `torch.nn.functional` has convolutions implemented for us. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnABzcOJl8-W"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "help(F.conv1d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWgWzxnImA7y"
      },
      "source": [
        "Notice `conv1d` expects\n",
        "1. `input` to have shape batchsize x #input_channels x length (as a warning, `tensorflow` expects batchsize x length x #input_channels!)\n",
        "2. `weight` (i.e. the filters) to have shape #output_channels x #input_channels x filter_width. \n",
        "\n",
        "We can make a random tensor for the filters (weights), and then execute the convolution "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDARAk-1matY"
      },
      "source": [
        "x_batch = x_tensor[None,:,:] # make a \"batch\" of size 1\n",
        "filter_width = 5 \n",
        "my_weights = torch.randn(1, 4, filter_width) # 1 output channels, 4 input channels, filter width = 5\n",
        "convolution_output = F.conv1d(x_batch, my_weights)\n",
        "convolution_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvEy9HNmyxt9"
      },
      "source": [
        "Let's check `torch` got the right answer. Here's the calculation for the first position (for data point 0 and output channel 0): "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh52sx3EygJG"
      },
      "source": [
        "(x_batch[0,:,0:filter_width] * my_weights[0,:,:]).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etX68ck0y-e5"
      },
      "source": [
        "## Task\n",
        "\n",
        "**Write a for loop to manually calculate the remaining terms in the convolution [1 point]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMUDwN30y9nM"
      },
      "source": [
        "my_convolution_output = torch.zeros([1, 1, x_batch.shape[2] - (filter_width-1)])\n",
        "for i in range(my_convolution_output.shape[2]): \n",
        "    my_convolution_output[0,0,i] = # TODO YOUR CODE HERE\n",
        "assert( ((convolution_output - my_convolution_output).abs() < 1e-6).all() )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfjjnIVrpHKs"
      },
      "source": [
        "Note the effect of the (0 padded) convolution on the shape of the output: the sequence length has gone from 14 to 10. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXnhZqIvpNMZ"
      },
      "source": [
        "print( x_batch.shape, convolution_output.shape )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUyEKqSsqpEQ"
      },
      "source": [
        "Without padding we lose (filter_width-1) positions. \n",
        "\n",
        "`torch` has nice built in utilities for setting up neural network (NN) layers. For example, we can make a 1D convolutional layer like this: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_RVwp-5jV6t"
      },
      "source": [
        "torch.manual_seed(2) # I played with different initialization here! \n",
        "my_first_conv_layer = nn.Conv1d(4, 1, 14, padding = 0) # 4 input channels, 1 output channels, filter width 14, no padding\n",
        "my_first_conv_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bnsi182bkcX"
      },
      "source": [
        "We can use this layer like a function (applied to tensors): "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Nqt2t07rjAT"
      },
      "source": [
        "my_first_conv_layer(x_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wzizdonsQOr"
      },
      "source": [
        "But if `my_first_conv_layer` behaves like a function, where are the weights? They're there (along with biases), but they're a little hidden: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TOiDTqolP8l"
      },
      "source": [
        "list(my_first_conv_layer.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZZijqVls2Sp"
      },
      "source": [
        "But where did these numbers come from? `torch` by default initializes using the very reasonable [Kaiming/He initalization](https://arxiv.org/abs/1502.01852) which aims to keep the variance of the hidden layer activations reasonably stable (constant in expectation) through the network. \n",
        "\n",
        "# My First Convolutional Neural Network\n",
        "\n",
        "We're ready to make the simplest CNN possible where we just take the max over all the convolution outputs: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN5TKgNus1Ob"
      },
      "source": [
        "def my_simplest_CNN(x): \n",
        "    net = my_first_conv_layer(x)\n",
        "    net = net[:,0,:] # only one output channel! \n",
        "    # take maximum over channel (\"global max pooling\")\n",
        "    net = torch.max(net, dim=1).values # max returns namedtuple (values, indices)\n",
        "    net = torch.sigmoid(net) # aka logistic to get output in [0,1]\n",
        "    return(net) \n",
        "\n",
        "my_simplest_CNN(x_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQGel5ea0RXI"
      },
      "source": [
        "## Loading data\n",
        "\n",
        "To test this out we need some data! Our first task will be predict binding of the important transcriptional repressor CTCF in a human lung cancer cell line called A549. The data is available from ENCODE including merging replicate experiments using the \"irreproducible discovery rate\" (IDR) [paper](https://arxiv.org/abs/1110.4705) [code](https://github.com/spundhir/idr). Genomics data representing discrete binding events is typically stored in the  `bed` format, which is described on the UCSC Genome Browser [website](https://genome.ucsc.edu/FAQ/FAQformat.html#format1). This bed file has some additional columns we will ignore. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0Kaptuq27QE"
      },
      "source": [
        "import pandas as pd\n",
        "DATADIR = \"/content/drive/My Drive/ML4fungen/Assignment 2/\" # might need to change this\n",
        "binding_data = pd.read_csv(DATADIR + \"ENCFF300IYQ.bed.gz\", sep='\\t', usecols=range(6), names=(\"chrom\",\"start\",\"end\",\"name\",\"score\",\"strand\"))\n",
        "binding_data = binding_data[ ~binding_data['chrom'].isin([\"chrX\",\"chrY\"]) ] # only keep autosomes (non sex chromosomes)\n",
        "binding_data = binding_data.sort_values(['chrom', 'start']).drop_duplicates() # sort so we can interleave negatives\n",
        "binding_data[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqbCc6Ii0X56"
      },
      "source": [
        "`chrom` is the chromosome (we filter out the sex chromosomes X and Y to avoid bias from these being haploid, you could try keeping them though). `start` and `end` are positions in the genome. `name` and `strand` aren't used here. Some genomics assays (e.g. RNA-seq) correspond to one strand of the DNA or the other. However, for our purposes at least we can consider ChIP-seq and ATAC-seq as \"unstranded\", which is why that column is all \".\" rather than + or -.  `score` is a quantitative measure of binding. We'll ignore this for now (since all these peaks are statistically significant), but you could try to incorporate it in your model training if you want (e.g. multitask for binary binding plus predicting `score`). \n",
        "\n",
        "We'll split the binding data into training, validation (chroms 2 and 3, ~) and test (chrom 1), which represents about 14% of the training data we have: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80imMbLs0Wxx"
      },
      "source": [
        "test_chromosomes = [\"chr1\"]\n",
        "test_data = binding_data[ binding_data['chrom'].isin( test_chromosomes ) ]\n",
        "\n",
        "validation_chromosomes = [\"chr2\",\"chr3\"]\n",
        "validation_data = binding_data[ binding_data['chrom'].isin(validation_chromosomes) ]\n",
        "\n",
        "train_chromosomes = [\"chr%i\" % i for i in range(4, 22+1)]\n",
        "train_data = binding_data[ binding_data['chrom'].isin( train_chromosomes ) ]\n",
        "\n",
        "test_data.shape[0] / binding_data.shape[0], validation_data.shape[0] / binding_data.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV_xNFIszaKf"
      },
      "source": [
        "We'll also need the human genome, which we provide here as a pickle since it's faster to load compared to reading in a text file. \n",
        "\n",
        "It's worth knowing that the human genome has different *versions* that are released as more missing parts are resolved by continued sequencing and assembly efforts. Version `GRCh37` (also called `hg19`) was released in 2009, and `GRCh38` (`hg38`) was released in 2013. We'll be using `hg19` here but `GRCh38` is finally becoming more standard so always check your data is what you think it is. \n",
        "\n",
        "This will take a minute or two. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fcHay8WzoOA"
      },
      "source": [
        "import pickle\n",
        "#genome = pickle.load(open(DATADIR+\"hg38.pkl\",\"rb\")) # this is here in case there's hg38 data you want to analyse\n",
        "genome = pickle.load(open(DATADIR+\"hg19.pickle\",\"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeoHEbQPzu7X"
      },
      "source": [
        "`genome` is just a dictionary where the keys are the chromosome names and the values are strings representing the actual DNA: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2zWSwlez8Ww"
      },
      "source": [
        "genome[\"chr13\"][100000000:100000010]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67O8pnWG12km"
      },
      "source": [
        "You'll find a substantial proportion of each chromosome is \"N\"s: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaq6pNv01phn"
      },
      "source": [
        "genome[\"chr13\"].count(\"N\") / len(genome[\"chr13\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wb_yFDe3n8q"
      },
      "source": [
        "Ns represents \"missing\" regions, typically because the region has too many repetitive sequences making mapping impossible, which is especially the case in [centrosomes](https://en.wikipedia.org/wiki/Centrosome) and [telomeres](https://en.wikipedia.org/wiki/Telomere). Resolving these difficult to map regions is an ongoing effort. \n",
        "\n",
        "We'll use the `torch` data loading utilities (nicely documented [here](https://pytorch.org/docs/stable/data.html)) to handle\n",
        "1. Grouping individual (x,y) pairs into minibatches. \n",
        "2. Converting `numpy` arrays into `torch` tensors. \n",
        "\n",
        "We could also use `num_workers>0` to have a background process generating the next batch using the CPU while the GPU is working, but in my experience this actually slows things down. If you were in a computer vision setting where you wanted to do intensive [data augmentation](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) it might make a difference. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-b1YTY4rvLiQ"
      },
      "source": [
        "# positive example: binding of protein onto sequence (ChIP-seq (TF ChIP-seq))\n",
        "# negative example: the ones that do not overlap with the positive examples \n",
        "# for chip-seq data: also shuffling nucleotides can be done to keep the GC content the same as positive example\n",
        "# because sequencing has biases with GC content and this would be a way to \"fix it\"\n",
        "class BedPeaksDataset(torch.utils.data.IterableDataset):\n",
        "\n",
        "    def __init__(self, atac_data, genome, context_length):\n",
        "        super(BedPeaksDataset, self).__init__()\n",
        "        self.context_length = context_length\n",
        "        self.atac_data = atac_data\n",
        "        self.genome = genome\n",
        "\n",
        "    def __iter__(self): \n",
        "        prev_end = 0\n",
        "        prev_chrom = \"\"\n",
        "        for i,row in enumerate(self.atac_data.itertuples()):\n",
        "            midpoint = int(.5 * (row.start + row.end))\n",
        "            seq = self.genome[row.chrom][ midpoint - self.context_length//2:midpoint + self.context_length//2]\n",
        "            yield(one_hot(seq), np.float32(1)) # positive example\n",
        "\n",
        "            if prev_chrom == row.chrom and prev_end < row.start: \n",
        "                midpoint = int(.5 * (prev_end + row.start))\n",
        "                seq = self.genome[row.chrom][ midpoint - self.context_length//2:midpoint + self.context_length//2]\n",
        "                yield(one_hot(seq), np.float32(0)) # negative example midway inbetween peaks, could randomize\n",
        "            \n",
        "            prev_chrom = row.chrom\n",
        "            prev_end = row.end\n",
        "\n",
        "train_dataset = BedPeaksDataset(train_data, genome, 100)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1000, num_workers = 0)\n",
        "\n",
        "train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO3iYoXd5BHR"
      },
      "source": [
        "## Training\n",
        "\n",
        "Let's train our super simple CNN using stochastic gradient descent: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2B52lJFuZML"
      },
      "source": [
        "import timeit\n",
        "start_time = timeit.default_timer()\n",
        "\n",
        "torch.set_grad_enabled(True) # we'll need gradients\n",
        "\n",
        "for epoch in range(10): # run for this many epochs\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    for (x,y) in train_dataloader: # iterate over minibatches\n",
        "\n",
        "        output = my_simplest_CNN(x) # forward pass\n",
        "        # in practice (and below) we'll use more numerically stable built-in\n",
        "        # functions for the loss\n",
        "        loss = - torch.mean( y * torch.log(output) + (1.-y) * torch.log(1.-output) )\n",
        "        loss.backward() # back propagation\n",
        "\n",
        "        # iterate over parameter tensors: just the layer1 weights and bias here\n",
        "        for parameters in my_first_conv_layer.parameters(): \n",
        "            parameters.data -= 1.0 * parameters.grad # in practive reduce or adapt learning rate\n",
        "            parameters.grad.data.zero_() # torch accumulates gradients so need to reset\n",
        "        \n",
        "        losses.append(loss.detach().numpy()) # convert back to numpy\n",
        "        accuracy = torch.mean( ( (output > .5) == (y > .5) ).float() )\n",
        "        accuracies.append(accuracy.detach().numpy())  \n",
        "\n",
        "    elapsed = float(timeit.default_timer() - start_time)\n",
        "    print(\"Epoch %i %.2fs/epoch Loss: %.4f Acc: %.4f\" % (epoch+1, elapsed/(epoch+1), np.mean(losses), np.mean(accuracies)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i62lB-k4usNq"
      },
      "source": [
        "So we can get pretty decent accuracy even with this very simple CNN (although I cheated a bit by trying a few random seeds and knowing that the CTCF consensus motif is 14nt long so would require a width 14 filter). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTZhOvaWsiki"
      },
      "source": [
        "validation_dataset = BedPeaksDataset(validation_data, genome, 100)\n",
        "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=1000)\n",
        "accuracies = [ torch.mean( ( (my_simplest_CNN(x)  > .5) == (y > .5) ).float() ).detach().cpu().numpy() for (x,y) in validation_dataloader ]\n",
        "np.mean(accuracies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He4O_i6JO1Ol"
      },
      "source": [
        "The validation accuracy is very similiar to the train accuracy, suggesting we're not overfitting (as we would expect with such a simple model). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuTlHyb9BNZ9"
      },
      "source": [
        "## Model interpretation\n",
        "\n",
        "Since this model used only a single convolutional filter it's very easy to interpret how it's making predictions. In genetics it's common to plot position weight matrices as \"sequence logos\": "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14eMljZ1Pg1y"
      },
      "source": [
        "!pip install logomaker\n",
        "import logomaker\n",
        "pwm = my_first_conv_layer.weight.detach().cpu().numpy().squeeze()\n",
        "pwm -= pwm.mean(0, keepdims=True) # remove spurious degrees of freedom\n",
        "pwm_df = pd.DataFrame(data = pwm.transpose(), columns=(\"A\",\"C\",\"G\",\"T\"))\n",
        "crp_logo = logomaker.Logo(pwm_df) # CCACCAGG(G/T)GGCG"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh01LaXnBcHA"
      },
      "source": [
        "Characters above (below) the x-axis correspond to positive (negative) values. Our model has some additional degrees of freedom (we could add a constant k to each column of the filter and substract k from the bias without changing the prediction) which we remove. Compare this to the \"known\" sequence motif: [JASPAR CTCF](http://jaspar.genereg.net/matrix/MA0139.1/) to see it's a pretty good match. \n",
        "\n",
        "# A full LeNet-type CNN\n",
        "\n",
        "Let's see if we can do better with a full CNN. While it's not strictly necessary it's convenient to encapsulate the model as a class inheriting from `nn.Module`. This allows automatic handling of things like: \n",
        "1. Extracting all model parameters. \n",
        "2. Setting all layers to train or eval mode (for layers like dropout that behave differently in the two settings). \n",
        "Note that here we used the model architecture to specify the length of sequence (the \"sequence context\") that is considered (stored as cnn_1d.seq_len). \"num_chunks\" here corresponds to the sequence length of the hidden layer after the last convolutional layer. The other common approach is to instead specify the sequence context and use the formula we covered in class to calculate the sequence length L for each each hidden layer (you're welcome to try implementing that approach!) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkyGN3fcbN6s"
      },
      "source": [
        "class CNN_1d(nn.Module):\n",
        "\n",
        "    def __init__(self, \n",
        "                 n_output_channels = 1, \n",
        "                 filter_widths = [15, 5], \n",
        "                 num_chunks = 5, \n",
        "                 max_pool_factor = 4, \n",
        "                 nchannels = [4, 32, 32],\n",
        "                 n_hidden = 32, \n",
        "                 dropout = 0.2):\n",
        "        \n",
        "        super(CNN_1d, self).__init__()\n",
        "        self.rf = 0 # running estimate of the receptive field\n",
        "        self.chunk_size = 1 # running estimate of num basepairs corresponding to one position after convolutions\n",
        "\n",
        "        conv_layers = []\n",
        "        for i in range(len(nchannels)-1):\n",
        "            conv_layers += [ nn.Conv1d(nchannels[i], nchannels[i+1], filter_widths[i], padding = 0),\n",
        "                        nn.BatchNorm1d(nchannels[i+1]), # tends to help give faster convergence: https://arxiv.org/abs/1502.03167\n",
        "                        nn.Dropout2d(dropout), # popular form of regularization: https://jmlr.org/papers/v15/srivastava14a.html\n",
        "                        nn.MaxPool1d(max_pool_factor), \n",
        "                        nn.ELU(inplace=True)  ] # popular alternative to ReLU: https://arxiv.org/abs/1511.07289\n",
        "            assert(filter_widths[i] % 2 == 1) # assume this\n",
        "            self.rf += (filter_widths[i] - 1) * self.chunk_size\n",
        "            self.chunk_size *= max_pool_factor\n",
        "\n",
        "        # If you have a model with lots of layers, you can create a list first and \n",
        "        # then use the * operator to expand the list into positional arguments, like this:\n",
        "        self.conv_net = nn.Sequential(*conv_layers)\n",
        "\n",
        "        self.seq_len = num_chunks * self.chunk_size + self.rf # amount of sequence context required\n",
        "\n",
        "        print(\"Receptive field:\", self.rf, \"Chunk size:\", self.chunk_size, \"Number chunks:\", num_chunks)\n",
        "\n",
        "        self.dense_net = nn.Sequential( nn.Linear(nchannels[-1] * num_chunks, n_hidden),\n",
        "                                        nn.Dropout(dropout),\n",
        "                                        nn.ELU(inplace=True), \n",
        "                                        nn.Linear(n_hidden, n_output_channels) )\n",
        "\n",
        "    def forward(self, x):\n",
        "        net = self.conv_net(x)\n",
        "        net = net.view(net.size(0), -1)\n",
        "        net = self.dense_net(net)\n",
        "        return(net)\n",
        "\n",
        "cnn_1d = CNN_1d()\n",
        "\n",
        "print(\"Input length:\", cnn_1d.seq_len)\n",
        "\n",
        "cnn_1d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8AYlNYgYzqu"
      },
      "source": [
        "The `torch` output nicely illustrates the various layers in our network. We have two convolutional units each doing conv -> batchnorm -> dropout -> maxpooling -> activation, followed by two layers of dense network, including a dropout layer.\n",
        "\n",
        "We'll recreate the dataloaders to satisfy the input sequence length requirement of the new model. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvKbQE_IYyZl"
      },
      "source": [
        "train_dataset = BedPeaksDataset(train_data, genome, cnn_1d.seq_len)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1000, num_workers = 0)\n",
        "validation_dataset = BedPeaksDataset(validation_data, genome, cnn_1d.seq_len)\n",
        "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evD6krFkC0PN"
      },
      "source": [
        "\n",
        "## Transfering to the GPU\n",
        "The previous example actually ran on the CPU. Now we're using a more complex model it'll be worth running on the GPU. To do that we need to transfer both model and data. Transfering the model: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOI-y0S_DYzY"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cnn_1d.to(device)\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWokUIAJDbmQ"
      },
      "source": [
        "Check that this says `cuda` not `cpu`! You need to go to Runtime -> Change runtime type -> Set \"Hardware Accelerator\" -> GPU (or TPU) if not. \n",
        "\n",
        "Next we'll set up the optimizer. `torch` has a number of [optimizers](https://pytorch.org/docs/stable/optim.html), most of which are variants of SGD. [Adam](https://arxiv.org/abs/1412.6980) with the [AMSgrad convergence fix](https://openreview.net/forum?id=ryQu7f-RZ) is a good default in my experience. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHy04RP7ES_M"
      },
      "source": [
        "optimizer = torch.optim.Adam(cnn_1d.parameters(), amsgrad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aeh19-CVEXb8"
      },
      "source": [
        "We define a training loop which can be used for both training and validation loops by setting the `train_flag`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndF6SLjkRd9e"
      },
      "source": [
        "def run_one_epoch(train_flag, dataloader, cnn_1d, optimizer, device=\"cuda\"):\n",
        "\n",
        "    torch.set_grad_enabled(train_flag)\n",
        "    cnn_1d.train() if train_flag else cnn_1d.eval() \n",
        "\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "\n",
        "    for (x,y) in dataloader: # collection of tuples with iterator\n",
        "\n",
        "        (x, y) = ( x.to(device), y.to(device) ) # transfer data to GPU\n",
        "\n",
        "        output = cnn_1d(x) # forward pass\n",
        "        output = output.squeeze() # remove spurious channel dimension\n",
        "        loss = F.binary_cross_entropy_with_logits( output, y ) # numerically stable\n",
        "\n",
        "        if train_flag: \n",
        "            loss.backward() # back propagation\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        losses.append(loss.detach().cpu().numpy())\n",
        "        accuracy = torch.mean( ( (output > .5) == (y > .5) ).float() )\n",
        "        accuracies.append(accuracy.detach().cpu().numpy())  \n",
        "    \n",
        "    return( np.mean(losses), np.mean(accuracies) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1ScQ2tqEqvm"
      },
      "source": [
        "## Training\n",
        "Ok let's train! We'll keep track of validation loss to do [early stopping](https://en.wikipedia.org/wiki/Early_stopping): if we don't see any improvement in the validation validation loss for 10 consecutive epochs then we stop training. This will take a few minutes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DAETm7kEsSr"
      },
      "source": [
        "train_accs = []\n",
        "val_accs = []\n",
        "patience = 10 # for early stopping\n",
        "patience_counter = patience\n",
        "best_val_loss = np.inf\n",
        "check_point_filename = 'cnn_1d_checkpoint.pt' # to save the best model fit to date\n",
        "for epoch in range(100):\n",
        "    start_time = timeit.default_timer()\n",
        "    train_loss, train_acc = run_one_epoch(True, train_dataloader, cnn_1d, optimizer, device)\n",
        "    val_loss, val_acc = run_one_epoch(False, validation_dataloader, cnn_1d, optimizer, device)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "    if val_loss < best_val_loss: \n",
        "        torch.save(cnn_1d.state_dict(), check_point_filename)\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = patience\n",
        "    else: \n",
        "        patience_counter -= 1\n",
        "        if patience_counter <= 0: \n",
        "            cnn_1d.load_state_dict(torch.load(check_point_filename)) # recover the best model so far\n",
        "            break\n",
        "    elapsed = float(timeit.default_timer() - start_time)\n",
        "    print(\"Epoch %i took %.2fs. Train loss: %.4f acc: %.4f. Val loss: %.4f acc: %.4f. Patience left: %i\" % \n",
        "          (epoch+1, elapsed, train_loss, train_acc, val_loss, val_acc, patience_counter ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4UCOvNnuIlI"
      },
      "source": [
        "Rather than trying to stare at those numbers let's plot training and validation accuracy: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoImu3R3Hn-4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_accs, label=\"train\")\n",
        "plt.plot(val_accs, label=\"validation\")\n",
        "plt.legend() \n",
        "plt.grid(which=\"both\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUf2GwIDUosU"
      },
      "source": [
        "Unusually the validation accuracy is higher than the train accuracy in some places! This could be due to (at least) two factors: \n",
        "1. Relatively small validation set so the estimate is noisy. \n",
        "2. Dropout introduces noise into the predictions during training but not validation (as it should). \n",
        "We could actually test this by evaluating on the training data without dropout: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l5YZkEZUoJn"
      },
      "source": [
        "train_loss, train_acc = run_one_epoch(False, train_dataloader, cnn_1d, optimizer, device)\n",
        "val_loss, val_acc = run_one_epoch(False, validation_dataloader, cnn_1d, optimizer, device)\n",
        "print(train_acc, val_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_GfVGD0CKNG"
      },
      "source": [
        "As expected, the validation accuracy is a little lower. Strictly speaking we should also check performance on the test set since we used the validation set to do early stopping. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tjgf38LjCVcV"
      },
      "source": [
        "test_dataset = BedPeaksDataset(test_data, genome, cnn_1d.seq_len)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1000)\n",
        "test_loss, test_acc = run_one_epoch(False, test_dataloader, cnn_1d, optimizer, device)\n",
        "test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsuxOfzFCiSp"
      },
      "source": [
        "Nice, the test set accuracy is high too. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEMyye5xUoSV"
      },
      "source": [
        "## Model interpretation\n",
        "\n",
        "Model interpretation is an active area of research for CNNs. Two baseline approaches in genomics are *in silico* mutagenesis and saliency maps. Good approaches exist for explaining the prediction for a single instance: explaining how the model works globally is still a challenge in general. \n",
        "\n",
        "### in silico mutatgenesis\n",
        "\n",
        "This approach is specific to genomics where we can imagine making individual \"point\" mutations (changing just one base) and seeing what effect that has on the model's prediction. In computer vision there isn't a direct analogy: deleting or changing a single pixel would rarely (if ever) change the prediction we would expect (although CNNs are not necessarily robust to such changes - adversarial training attempts to make them so).  \n",
        "\n",
        "*in silico* mutagenesis can be used both for model interpretation (as we do here) and predicting the effects of real mutations/genetic differences between individuals. \n",
        "\n",
        "We'll just run mutagenesis for the first batch of the validation data: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvSmKbVKO38J"
      },
      "source": [
        "torch.set_grad_enabled(False)\n",
        "for (x_cpu,y_cpu) in validation_dataloader: \n",
        "    x = x_cpu.to(device)\n",
        "    y = y_cpu.to(device)\n",
        "    output = cnn_1d(x).squeeze()\n",
        "    output = torch.sigmoid(output)\n",
        "    delta_output = torch.zeros_like(x, device=device)\n",
        "    # loop over all positions changing to each position nucleotide\n",
        "    # note everything is implicitly parallelized over the batch here\n",
        "    for seq_idx in range(cnn_1d.seq_len): # iterate over sequence\n",
        "        for nt_idx in range(4): # iterate over nucleotides\n",
        "            x_prime = x.clone() # make a copy of x\n",
        "            x_prime[:,:,seq_idx] = 0. # change the nucleotide to nt_idx\n",
        "            x_prime[:,nt_idx,seq_idx] = 1.\n",
        "            output_prime = cnn_1d(x_prime).squeeze()\n",
        "            output_prime = torch.sigmoid(output_prime)\n",
        "            delta_output[:,nt_idx,seq_idx] = output_prime - output\n",
        "    break # just do this for first batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVPr0R2UI5_6"
      },
      "source": [
        "Note how computationally expensive this is: for every instance we do inference $4 \\times L$ times (we could make this $3 \\times L$ easily enough). \n",
        "\n",
        "We'll visualize just four (2 positive 2 negative) examples: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usc1RHqHiJgI"
      },
      "source": [
        "delta_output_np = delta_output.detach().cpu().numpy()\n",
        "delta_output_np -= delta_output_np.mean(1, keepdims=True)\n",
        "output_np = output.detach().cpu().numpy()\n",
        "plt.figure(figsize = (12,12))\n",
        "for i in range(1,5):\n",
        "    ax = plt.subplot(4,1,i)\n",
        "    pwm_df = pd.DataFrame(data = delta_output_np[i,:,:].transpose(), columns=(\"A\",\"C\",\"G\",\"T\"))\n",
        "    crp_logo = logomaker.Logo(pwm_df, ax = ax) # CCGCGNGGNGGCAG or CTGCCNCCNCGCGG\n",
        "    plt.title(\"True label: %i. Prob(y=1)=%.3f\" % (y_cpu[i],output_np[i]))\n",
        "\n",
        "plt.subplots_adjust(hspace = 0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_k64EEewJUYC"
      },
      "source": [
        "Note the difference in the y-axis scales! For the two positive examples there are clear regions in the sequence that if disrupted significantly impact the prediction. For the negative examples there is a scattering of \"mutations\" that would effect the prediction, presumably by randomly introducing a sequence that looks a little like the CTCF motif (we don't expect any specific features in these negative sequences). \n",
        "\n",
        "### Saliency maps\n",
        "\n",
        "This is the simplest approach to leverage the same backprop machinery that we use during training. The trick is that instead of taking the gradient w.r.t. parameters we'll now take the gradient w.r.t. inputs `x`. Specifically here we'll get the gradient of $P(y=1|x)$ w.r.t x. The `saliency` itself is defined as that gradient elementwise multiplied with `x` itself. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJZI94jdAYW0"
      },
      "source": [
        "torch.set_grad_enabled(True)\n",
        "x.requires_grad_() # tell torch we will want gradients wrt x (which we don't normally need)\n",
        "output = cnn_1d(x).squeeze()\n",
        "output = torch.sigmoid(output)\n",
        "dummy = torch.ones_like(output) # in a multiclass model this would be a one-hot encoding of y\n",
        "output.backward(dummy) # to get derivative wrt to x\n",
        "gradient_np = x.grad.detach().cpu().numpy()\n",
        "output_np = output.detach().cpu().numpy()\n",
        "saliency = gradient_np * x_cpu.numpy()\n",
        "plt.figure(figsize = (12,12))\n",
        "for i in range(1,5):\n",
        "    ax = plt.subplot(4,1,i) #,sharey=ax)\n",
        "    pwm_df = pd.DataFrame(data = saliency[i,:,:].transpose(), columns=(\"A\",\"C\",\"G\",\"T\"))\n",
        "    logomaker.Logo(pwm_df, ax=ax) # CCGCGNGGNGGCAG or CTGCCNCCNCGCGG\n",
        "    plt.title(\"True label: %i. Prob(y=1)=%.3f\" % (y_cpu[i],output_np[i]))\n",
        "\n",
        "plt.subplots_adjust(hspace = 0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKUdaHbpMa6l"
      },
      "source": [
        "This is much more computationally efficient than in silico mutagenesis, requiring only ONE forward and backward pass. Compare the two interpretation methods: for the positive examples the same sequence regions are highlighted (although the saliency map is noisier outside that region). \n",
        "\n",
        "### Other interpretation approaches\n",
        "\n",
        "There's a nice compedium and associated `torch` code for a number methods [here](https://github.com/utkuozbulak/pytorch-cnn-visualizations). Some that I would add: \n",
        "1. DeepLIFT  [paper](https://arxiv.org/abs/1704.02685) [code](https://github.com/kundajelab/deeplift) - no `pytorch` support sadly. \n",
        "2. DeepSHAP [paper](https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions) [code](https://github.com/slundberg/shap) \n",
        "3. Influence functions. [paper](https://arxiv.org/abs/1703.04730) [code](https://github.com/kohpangwei/influence-release). A little different: finds which training points most strongly influence the current prediction. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgEkRJYcTqto"
      },
      "source": [
        "## A more challenging task\n",
        "\n",
        "CTCF binding is relatively easy to predict, mostly depending on homotypic binding with a strong motif. Here you'll attempt a more difficult task: predicting chromatin accessibility (as measured by ATAC-seq) for a mystery cell type (mysterious so you can fairly compete in the class competition below!) Let's load data and split into training and validation - I've already removed chromosome 1 and 2 data as test set to be used in the competition. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prt0Jug2Twfm"
      },
      "source": [
        "atac_data = pd.read_csv(DATADIR + \"ATAC_data.bed.gz\", sep='\\t', names=(\"chrom\",\"start\",\"end\"))\n",
        "atac_data = atac_data.sort_values(['chrom', 'start']) # actually already sorted but why not\n",
        "\n",
        "validation_chromosomes = [\"chr3\",\"chr4\"]\n",
        "validation_data = atac_data[ atac_data['chrom'].isin(validation_chromosomes) ]\n",
        "\n",
        "train_data = atac_data[ ~atac_data['chrom'].isin( validation_chromosomes ) ]\n",
        "\n",
        "train_data.shape[0] / atac_data.shape[0], validation_data.shape[0] / atac_data.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzYYZ7XWHokb"
      },
      "source": [
        "# Tasks\n",
        "\n",
        "Make your own copy of this notebook complete the following questions by filling in the code and write-up sections. Feel free to add cells as needed.\n",
        "\n",
        "## Wrapper function [2 points]\n",
        "\n",
        "It will be helpful to make a wrapper function that does the following: \n",
        "1. Make new `BedPeakDataset` and `DataLoader` objects for both training and validation data. \n",
        "2. Instantiates an optimizer for the model. \n",
        "3. Runs the training loop with early stopping. \n",
        "4. Returns the fitted model, train and validation accuracies.\n",
        "\n",
        "We've given a suggested signature for the wrapper function. You'll find the code snippets you need above although you will need to do some minor editing if you want the optional arguments to have the correct effects. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhDEJy1-iphQ"
      },
      "source": [
        "# TODO make wrapper function. \n",
        "\n",
        "def train_model(cnn_1d, train_data, validation_data, epochs=100, patience=10, verbose = True):\n",
        "    \"\"\"\n",
        "    Train a 1D CNN model and record accuracy metrics.\n",
        "    \"\"\"\n",
        "    # Move the model to the GPU here to make it runs there, and set \"device\" as above\n",
        "    # TODO CODE\n",
        "\n",
        "    # 1. Make new BedPeakDataset and DataLoader objects for both training and validation data.\n",
        "    # TODO CODE\n",
        "\n",
        "    # 2. Instantiates an optimizer for the model. \n",
        "    # TODO CODE\n",
        "\n",
        "    # 3. Run the training loop with early stopping. \n",
        "    # TODO CODE\n",
        "\n",
        "    # 4. Return the fitted model (not strictly necessary since this happens \"in place\"), train and validation accuracies.\n",
        "    # TODO CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEivvnzFKpW8"
      },
      "source": [
        "You should now be able to train a basic CNN with the same architechure as we used above for CTCF binding prediction. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T74hQno-82Ty"
      },
      "source": [
        "my_cnn1d = CNN_1d()\n",
        "print(my_cnn1d.seq_len)\n",
        "my_cnn1d\n",
        "my_cnn1d, train_accs, val_accs = train_model(my_cnn1d, train_data, validation_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J44YB2ZjK7z2"
      },
      "source": [
        "## Question 1 [4 points]\n",
        "\n",
        "a. Find settings of `CNN_1d` that underfit (low train and test accuracy) and plot the train and validation accuracy. [3 points]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu4SVPLuATEj"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLkHsofr7_G0"
      },
      "source": [
        "b. Describe the setting choices you have made to underfit the data and explain why these settings contributed to a low train and test accuracy. [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ATP24Bg8QzI"
      },
      "source": [
        "*Fill in with your explanation. Feel free to add any plots or tables if you feel they will be helpful.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE5QtGHe8kfQ"
      },
      "source": [
        "## Question 2 [4 points]\n",
        "\n",
        "a. Find settings of `CNN_1d` that that overfit (high train accuracy but low test accuracy). [3 points]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjmOqmQdCGf2"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KFkx1QF8sBK"
      },
      "source": [
        "b. Describe the setting choices you have made to overfit the data and explain why these settings contributed to a high train and low test accuracy. [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5S9jEmR9Yv0"
      },
      "source": [
        "*Fill in with your explanation. Feel free to add any plots or tables if you feel they will be helpful.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5S9qMVo9h3k"
      },
      "source": [
        "## Question 3  [6 points]\n",
        "\n",
        "a. Carefully explore varying one architectural choice (e.g.  depth, number of channels, filter width, regularization, pooling factor, optimizer, learning rate, batch size, activation function, normalization, or skip connections). Report the final train and validation accuracy as a function of this choice. \n",
        "\n",
        "For example you might vary the number of convolutional layers from 1 to 4, keeping everything else the same, and plot validation accuracy vs number of layers. [4 points]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6zzCTyUFGLK"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_SR-Td1FHCw"
      },
      "source": [
        "b. For your selected architectural choice, discuss how and why varying this option affected your training and validation accuracy. [2 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsTXc63mFhQs"
      },
      "source": [
        "*Fill in with your explanation. Feel free to add any plots or tables if you feel they will be helpful.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn74_zReHgh5"
      },
      "source": [
        "## Question 4 [8 points]\n",
        "\n",
        "a. Get as good validation accuracy as you can! [4 points]\n",
        "\n",
        "Optionally you can try some more advanced extensions, e.g. \n",
        "1. Adding some [Recurrent Layers](https://pytorch.org/docs/stable/nn.html#recurrent-layers). Be warned that `torch` assumes the opposite dimensions for convolutional vs recurrent layers so you'll want to use `torch.transpose` appropriately. \n",
        "2. Transcription factors can bind to either strand of the DNA so you might want to include the [reverse complement](https://www.bx.psu.edu/old/courses/bx-fall08/definitions.html) (RC) in addition to the normal input. One suggestion for how to do this: run a copy of the network on the RC and take the max of the output from the two networks. Fancier approach here: https://www.biorxiv.org/content/10.1101/103663v1\n",
        "3. Changing `BedPeaksDataset` to generate more negative examples in the space between peaks (although you'd only want to do this on the training data so you keep a fixed validation set). You will want to \"cancel out\" the additional negative examples by downweighting them in the loss. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_Aog3UmHlVE"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLBwoDdOqCg5"
      },
      "source": [
        "b. What gave you the biggest boost in performance? Why do you think that is? [2 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UWhYVEaqCV8"
      },
      "source": [
        "*Fill in with your explanation. Feel free to add any plots or tables if you feel they will be helpful.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSoUa7xcJKto"
      },
      "source": [
        "c.  Use some model interpretability technique to visualize why the model has made the assignments it did for a few examples. This can be one of the methods shown above (in silico mutagenesis or saliency maps) or one of the methods linked under *Other interpretation approaches*. [2 points]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0fLH5VqJLdf"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RIZyYe4iwBy"
      },
      "source": [
        "## Question 5  [6 points]\n",
        "\n",
        "The final task is to submit your best performing model's predictions on chromosomes 1 and 2 to our class competition. \n",
        "\n",
        "You can change what train/validation split you use here also if you want: e.g. you could do K-fold cross-validation or even retrain including the validation data if you think it will help (although early stopping may not work any more). \n",
        "\n",
        "First we'll need to load the test regions: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Pk4Z09Ml2zc"
      },
      "source": [
        "test_data = pd.read_csv(DATADIR + \"ATAC_test_regions.bed.gz\", sep='\\t', names=(\"chrom\",\"start\",\"end\"))\n",
        "test_data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w85FstImJBq"
      },
      "source": [
        "Unlike the training data we've included random (in number and in genomic position) negative (no binding) regions in this bed file since otherwise you'd know implicitly that all the loaded regions are positives! \n",
        "\n",
        "We'll use a new `Dataset` class and `DataLoader` to match predictions on the test data in batches: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNkgageAl5aC"
      },
      "source": [
        "class BedPeaksDatasetTest(torch.utils.data.IterableDataset):\n",
        "    \n",
        "    def __init__(self, atac_data, genome, context_length):\n",
        "        super(BedPeaksDatasetTest, self).__init__()\n",
        "        self.context_length = context_length\n",
        "        self.atac_data = atac_data\n",
        "        self.genome = genome\n",
        "\n",
        "    def __iter__(self): \n",
        "        for row in self.atac_data.itertuples():\n",
        "            midpoint = int(.5 * (row.start + row.end))\n",
        "            seq = self.genome[row.chrom][ midpoint - self.context_length//2:midpoint + self.context_length//2]\n",
        "            yield(one_hot(seq))\n",
        "\n",
        "test_dataset = BedPeaksDatasetTest(test_data, genome, my_cnn1d.seq_len) \n",
        "# you can always use a smaller batchsize if you ended up using a really big model\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, num_workers = 0) \n",
        "\n",
        "outputs = []\n",
        "for x in test_dataloader: # iterate over batches\n",
        "    x = x.to(device)\n",
        "    output = my_cnn1d(x).squeeze() # your awesome model here! \n",
        "    output = torch.sigmoid(output)\n",
        "    output_np = output.detach().cpu().numpy()\n",
        "    outputs.append(output_np)\n",
        "output_np = np.concatenate(outputs)\n",
        "\n",
        "predicted_values=output_np.tolist() # Create list of your predictions from test data\n",
        "pd.DataFrame(output_np).to_csv(\"path/to/file.csv\") # Save to csv file\n",
        "predicted_values[0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH8xt07Ym5Bl"
      },
      "source": [
        "Now you can make a submission to our class competition leaderboard! \n",
        "\n",
        "1. Sign up for a username and password at www.modelshare.org/login. \n",
        "2. Make sure your username is your Columbia Uni, so we know which submissions are yours. \n",
        "3. Use the below code to submit your predictions to the class [leaderboard](https://www.modelshare.org/detail/model:1484) ***(click compete tab)***\n",
        "\n",
        "For the assignment you only need to make one leaderboard submission, but you can upload two submissions per day until the deadline if you want! \n",
        "\n",
        "Submission is worth 2 points, beating our baseline (auPR=0.82369) gets you 2 points and the other 2 points are for your tertile in the class (e.g. you get 2 points if you're in the top third, 1 point if you're in the middle third). "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install aimodelshare library\n",
        "! pip install aimodelshare --upgrade"
      ],
      "metadata": {
        "id": "hhXFYv1PK8Bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set credentials using modelshare.org username/password\n",
        "from aimodelshare.aws import set_credentials\n",
        "\n",
        "#Enter your modelshare.org username and password\n",
        "apiurl=\"https://yro0pg9wja.execute-api.us-east-2.amazonaws.com/prod/m\"\n",
        "set_credentials(apiurl=apiurl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQ4MuscpNUim",
        "outputId": "5e13b3c7-5515-4de2-f99e-e3926a121d01"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Modelshare Username:\n",
            "AI Modelshare Password:\n",
            "AI Model Share login credentials set successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Instantiate Competition\n",
        "import aimodelshare as ai\n",
        "mycompetition= ai.Competition(apiurl)"
      ],
      "metadata": {
        "id": "-YpDut0pNlt2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Submit Model predictions to leaderboard (without extracting model architecture information): \n",
        "\n",
        "# Submit Model 1 predictions to Competition Leaderboard\n",
        "mycompetition.submit_model(model_filepath = None,\n",
        "                                 preprocessor_filepath=None,\n",
        "                                 prediction_submission=predicted_values)\n",
        "\n",
        "# Repeat with new predictions to submit again!"
      ],
      "metadata": {
        "id": "zsmDzhE7N_K8",
        "outputId": "3086b5cf-e51e-4271-8da5-3c8fa9698108",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Insert search tags to help users find your model (optional): \n",
            "Provide any useful notes about your model (optional): \n",
            "\n",
            "Your model has been submitted as model version 6\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get leaderboard to check your performance\n",
        "\n",
        "# Get raw data in pandas data frame\n",
        "data = mycompetition.get_leaderboard()\n",
        "\n",
        "# Stylize leaderboard data\n",
        "mycompetition.stylize_leaderboard(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "yl5SBTRyOVzD",
        "outputId": "cdce3b6c-6ba8-4596-cc47-22e204bcdd85"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f946f2dd710>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_d89fd_row0_col0, #T_d89fd_row1_col0 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#f5f8d6 49.5%, transparent 49.5%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d89fd_row0_col1, #T_d89fd_row1_col1 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#c778c8 48.2%, transparent 48.2%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d89fd_row0_col2, #T_d89fd_row1_col2 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#ff4971 49.3%, transparent 49.3%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d89fd_row0_col3, #T_d89fd_row1_col3 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#aadbaa 49.2%, transparent 49.2%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d89fd_row0_col4, #T_d89fd_row0_col5, #T_d89fd_row0_col6, #T_d89fd_row0_col7, #T_d89fd_row0_col8, #T_d89fd_row1_col4, #T_d89fd_row1_col5, #T_d89fd_row1_col6, #T_d89fd_row1_col7, #T_d89fd_row1_col8, #T_d89fd_row2_col4, #T_d89fd_row2_col5, #T_d89fd_row2_col6, #T_d89fd_row2_col7, #T_d89fd_row2_col8, #T_d89fd_row3_col4, #T_d89fd_row3_col5, #T_d89fd_row3_col6, #T_d89fd_row3_col7, #T_d89fd_row3_col8, #T_d89fd_row4_col4, #T_d89fd_row4_col5, #T_d89fd_row4_col6, #T_d89fd_row4_col7, #T_d89fd_row4_col8, #T_d89fd_row5_col4, #T_d89fd_row5_col5, #T_d89fd_row5_col6, #T_d89fd_row5_col7, #T_d89fd_row5_col8 {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_d89fd_row2_col0, #T_d89fd_row4_col0, #T_d89fd_row5_col0 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#f5f8d6 50.0%, transparent 50.0%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d89fd_row2_col1, #T_d89fd_row4_col1, #T_d89fd_row5_col1 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#c778c8 48.7%, transparent 48.7%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d89fd_row2_col2, #T_d89fd_row4_col2, #T_d89fd_row5_col2 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#ff4971 49.8%, transparent 49.8%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d89fd_row2_col3, #T_d89fd_row4_col3, #T_d89fd_row5_col3 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#aadbaa 49.7%, transparent 49.7%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d89fd_row3_col0 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#f5f8d6 50.3%, transparent 50.3%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d89fd_row3_col1 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#c778c8 49.0%, transparent 49.0%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d89fd_row3_col2 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#ff4971 50.0%, transparent 50.0%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_d89fd_row3_col3 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#aadbaa 50.0%, transparent 50.0%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_d89fd_\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" >accuracy</th>\n",
              "      <th class=\"col_heading level0 col1\" >f1_score</th>\n",
              "      <th class=\"col_heading level0 col2\" >precision</th>\n",
              "      <th class=\"col_heading level0 col3\" >recall</th>\n",
              "      <th class=\"col_heading level0 col4\" >auPR</th>\n",
              "      <th class=\"col_heading level0 col5\" >ml_framework</th>\n",
              "      <th class=\"col_heading level0 col6\" >model_type</th>\n",
              "      <th class=\"col_heading level0 col7\" >username</th>\n",
              "      <th class=\"col_heading level0 col8\" >version</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_d89fd_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_d89fd_row0_col0\" class=\"data row0 col0\" >49.47%</td>\n",
              "      <td id=\"T_d89fd_row0_col1\" class=\"data row0 col1\" >48.19%</td>\n",
              "      <td id=\"T_d89fd_row0_col2\" class=\"data row0 col2\" >49.31%</td>\n",
              "      <td id=\"T_d89fd_row0_col3\" class=\"data row0 col3\" >49.24%</td>\n",
              "      <td id=\"T_d89fd_row0_col4\" class=\"data row0 col4\" >0.502448</td>\n",
              "      <td id=\"T_d89fd_row0_col5\" class=\"data row0 col5\" >unknown</td>\n",
              "      <td id=\"T_d89fd_row0_col6\" class=\"data row0 col6\" >unknown</td>\n",
              "      <td id=\"T_d89fd_row0_col7\" class=\"data row0 col7\" >newusertest</td>\n",
              "      <td id=\"T_d89fd_row0_col8\" class=\"data row0 col8\" >5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d89fd_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_d89fd_row1_col0\" class=\"data row1 col0\" >49.47%</td>\n",
              "      <td id=\"T_d89fd_row1_col1\" class=\"data row1 col1\" >48.19%</td>\n",
              "      <td id=\"T_d89fd_row1_col2\" class=\"data row1 col2\" >49.31%</td>\n",
              "      <td id=\"T_d89fd_row1_col3\" class=\"data row1 col3\" >49.24%</td>\n",
              "      <td id=\"T_d89fd_row1_col4\" class=\"data row1 col4\" >0.502448</td>\n",
              "      <td id=\"T_d89fd_row1_col5\" class=\"data row1 col5\" >unknown</td>\n",
              "      <td id=\"T_d89fd_row1_col6\" class=\"data row1 col6\" >unknown</td>\n",
              "      <td id=\"T_d89fd_row1_col7\" class=\"data row1 col7\" >newusertest</td>\n",
              "      <td id=\"T_d89fd_row1_col8\" class=\"data row1 col8\" >6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d89fd_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_d89fd_row2_col0\" class=\"data row2 col0\" >49.97%</td>\n",
              "      <td id=\"T_d89fd_row2_col1\" class=\"data row2 col1\" >48.67%</td>\n",
              "      <td id=\"T_d89fd_row2_col2\" class=\"data row2 col2\" >49.77%</td>\n",
              "      <td id=\"T_d89fd_row2_col3\" class=\"data row2 col3\" >49.74%</td>\n",
              "      <td id=\"T_d89fd_row2_col4\" class=\"data row2 col4\" >nan</td>\n",
              "      <td id=\"T_d89fd_row2_col5\" class=\"data row2 col5\" >unknown</td>\n",
              "      <td id=\"T_d89fd_row2_col6\" class=\"data row2 col6\" >unknown</td>\n",
              "      <td id=\"T_d89fd_row2_col7\" class=\"data row2 col7\" >newusertest</td>\n",
              "      <td id=\"T_d89fd_row2_col8\" class=\"data row2 col8\" >1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d89fd_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_d89fd_row3_col0\" class=\"data row3 col0\" >50.31%</td>\n",
              "      <td id=\"T_d89fd_row3_col1\" class=\"data row3 col1\" >48.97%</td>\n",
              "      <td id=\"T_d89fd_row3_col2\" class=\"data row3 col2\" >50.01%</td>\n",
              "      <td id=\"T_d89fd_row3_col3\" class=\"data row3 col3\" >50.01%</td>\n",
              "      <td id=\"T_d89fd_row3_col4\" class=\"data row3 col4\" >nan</td>\n",
              "      <td id=\"T_d89fd_row3_col5\" class=\"data row3 col5\" >unknown</td>\n",
              "      <td id=\"T_d89fd_row3_col6\" class=\"data row3 col6\" >unknown</td>\n",
              "      <td id=\"T_d89fd_row3_col7\" class=\"data row3 col7\" >mikedparrott</td>\n",
              "      <td id=\"T_d89fd_row3_col8\" class=\"data row3 col8\" >2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d89fd_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_d89fd_row4_col0\" class=\"data row4 col0\" >49.97%</td>\n",
              "      <td id=\"T_d89fd_row4_col1\" class=\"data row4 col1\" >48.67%</td>\n",
              "      <td id=\"T_d89fd_row4_col2\" class=\"data row4 col2\" >49.77%</td>\n",
              "      <td id=\"T_d89fd_row4_col3\" class=\"data row4 col3\" >49.74%</td>\n",
              "      <td id=\"T_d89fd_row4_col4\" class=\"data row4 col4\" >nan</td>\n",
              "      <td id=\"T_d89fd_row4_col5\" class=\"data row4 col5\" >unknown</td>\n",
              "      <td id=\"T_d89fd_row4_col6\" class=\"data row4 col6\" >unknown</td>\n",
              "      <td id=\"T_d89fd_row4_col7\" class=\"data row4 col7\" >newusertest</td>\n",
              "      <td id=\"T_d89fd_row4_col8\" class=\"data row4 col8\" >3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d89fd_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_d89fd_row5_col0\" class=\"data row5 col0\" >49.97%</td>\n",
              "      <td id=\"T_d89fd_row5_col1\" class=\"data row5 col1\" >48.67%</td>\n",
              "      <td id=\"T_d89fd_row5_col2\" class=\"data row5 col2\" >49.77%</td>\n",
              "      <td id=\"T_d89fd_row5_col3\" class=\"data row5 col3\" >49.74%</td>\n",
              "      <td id=\"T_d89fd_row5_col4\" class=\"data row5 col4\" >nan</td>\n",
              "      <td id=\"T_d89fd_row5_col5\" class=\"data row5 col5\" >unknown</td>\n",
              "      <td id=\"T_d89fd_row5_col6\" class=\"data row5 col6\" >unknown</td>\n",
              "      <td id=\"T_d89fd_row5_col7\" class=\"data row5 col7\" >newusertest</td>\n",
              "      <td id=\"T_d89fd_row5_col8\" class=\"data row5 col8\" >4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**(Optional Extension) Submit Model With Custom Metadata:**\n",
        "\n",
        "Can use to add team names or any other missing data you may wish to share on the leaderboard\n"
      ],
      "metadata": {
        "id": "YcMFmyNXPUJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom metadata can be added by passing a dict to the custom_metadata argument of the submit_model() method\n",
        "# This option can be used to fill in missing data points or add new columns to the leaderboard\n",
        "\n",
        "custom_meta = {'team': 'team one',\n",
        "               'model_type': 'your_model_type',\n",
        "               'new_column': 'new metadata'}\n",
        "\n",
        "mycompetition.submit_model(model_filepath = None,\n",
        "                                 preprocessor_filepath=None,\n",
        "                                 prediction_submission=predicted_values,\n",
        "                                 custom_metadata = custom_meta)\n",
        "\n"
      ],
      "metadata": {
        "id": "fVOrHuLRPWu0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}