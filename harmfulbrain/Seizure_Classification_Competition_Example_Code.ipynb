{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSx_wHMY6KaL"
      },
      "source": [
        "<p align=\"center\"><img width=\"100%\" src='https://drive.google.com/uc?id=1ecTqgg55aswLO-s1vqG2fTyeRoo7pmJ5'></p>"
      ],
      "id": "jSx_wHMY6KaL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fd41e0f-9b75-475d-9328-b7e524b87c7b"
      },
      "source": [
        "\n",
        "## **Competition Example Code:**  \n",
        "### 1. Train Example Model using training dataset\n",
        "### 2. Generate Predictions from X test data\n",
        "### 3. Submit to competition leaderboard\n",
        "\n"
      ],
      "id": "4fd41e0f-9b75-475d-9328-b7e524b87c7b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uhg7N0s98zUb"
      },
      "source": [
        "## 1. Use Google Drive link to view a folder.  Please request access via email.\n",
        "https://drive.google.com/drive/folders/1pQtbHymHwvBups8jvGgIuaWBxGXRP0Ie?usp=sharing\n",
        "\n",
        "***The competition training data is stored in four zipfiles in the TrainingSet folder:***\n",
        "\n",
        "TrainingSet\n",
        "\n",
        "*   trainbatch01.zip\n",
        "*   trainbatch02.zip\n",
        "*   trainbatch03.zip\n",
        "*   trainbatch04.zip\n",
        "\n",
        "## 2. From Google Drive, right click the folder name (\"Harvard Hospital\") and click \"Add shortcut to Drive\" after receiving access\n",
        "\n",
        "Google Colab users can directly connect to this Google Drive folder and begin model training with Colab by following instructions to mount this drive below.\n",
        "\n",
        "This will be sure the data in this folder is accessible in your personal drive folder.\n",
        "\n",
        "If you do not wish to use Colab, you can skip this step and simply download the competition data and train your model with your preferred approach.\n",
        "\n",
        "<p align=\"center\"><img width=\"40%\" src='https://drive.google.com/uc?id=1WTeV9qblf19IpPpW0ejXMFTeZlftn29v'></p>\n"
      ],
      "id": "Uhg7N0s98zUb"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dDmkXP_6FpG",
        "outputId": "551d70e0-de25-43d1-b21f-3586754fc0db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Connect to your Google Drive to train models using Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "7dDmkXP_6FpG"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM5WdkbttiZR",
        "outputId": "b2dcd833-27c1-47c8-81fd-be880b51c437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'harvarddatautils'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (6/6), 4.25 KiB | 4.25 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# Download helper files from github repo and copy to working directory\n",
        "!git clone https://github.com/AIModelShare/harvarddatautils\n"
      ],
      "id": "dM5WdkbttiZR"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JnwR7Pumvmpv",
        "outputId": "02f24e2b-a397-4722-bc9a-04e0c5172a7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Copy utility files to working directory\n",
        "import shutil\n",
        "import os\n",
        " \n",
        "# path to source directory\n",
        "src_dir = 'harvarddatautils'\n",
        " \n",
        "# path to destination directory\n",
        "dest_dir = os.getcwd()\n",
        " \n",
        "shutil.copytree(src_dir, dest_dir,dirs_exist_ok=True)"
      ],
      "id": "JnwR7Pumvmpv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please first run `bash init.sh` to install the dependencies.\n"
      ],
      "metadata": {
        "id": "fUhOFKoIVJVY"
      },
      "id": "fUhOFKoIVJVY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uYXrh2B-3D1"
      },
      "outputs": [],
      "source": [
        "#install missing libraries using init.sh help file\n",
        "!bash init.sh"
      ],
      "id": "4uYXrh2B-3D1"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "14314b8f-4c75-41df-b032-2864925a2f10"
      },
      "outputs": [],
      "source": [
        "from importlib import reload\n",
        "import datetime\n",
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "DEBUG = True\n",
        "CHECKPOINT_DIR = os.path.join('checkpoints')\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# Note - Adjust this filepath to the directory where your training zipfiles are saved.\n",
        "trainDir = \"drive/MyDrive/Harvard Hospital/trainingSet\""
      ],
      "id": "14314b8f-4c75-41df-b032-2864925a2f10"
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract a batch of training data from zipfile to working directory\n",
        "# Use this batch of data to train a model.\n",
        "\n",
        "# Extract more batches to continue to train model on more data.\n",
        "\n",
        "from zipfile import ZipFile\n",
        "  \n",
        "# extracting training batch to working directory into folder named \"batch01\"\n",
        "with ZipFile(trainDir+\"/trainbatch1.zip\", 'r') as zObject:\n",
        "    zObject.extractall(\n",
        "        path=os.getcwd())\n",
        "    \n",
        "#Data is now extracted to \"batch01\" folder!  \n",
        "#Change zipfile name to trainbatch2.zip, trainbatch3.zip, or trainbatch4.zip to extract more training data.\n"
      ],
      "metadata": {
        "id": "SFtJ3wJb9s07"
      },
      "id": "SFtJ3wJb9s07",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83e73847-1e73-4cf0-954d-9e3ce3d44ab4"
      },
      "source": [
        "## Examine Data"
      ],
      "id": "83e73847-1e73-4cf0-954d-9e3ce3d44ab4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Example data file path\n",
        "path=os.listdir(\"batch01\")[0]\n",
        "print(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL5YmIIH9Yvq",
        "outputId": "9405fefc-7dfe-47a8-dcb7-3dc43d956f90"
      },
      "id": "ZL5YmIIH9Yvq",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample043265.mat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R35x-wIkdY8P",
        "outputId": "6636e45a-5172-417c-e6a9-a558511aeef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# Check if file exists - You may have to adjust the \"trainDir\" filepath above to point to the location where you saved the shortcut in step 1\n",
        "import os.path\n",
        "\n",
        "path = os.path.join(\"batch01\", path)\n",
        "check_file = os.path.isfile(path)\n",
        "\n",
        "print(check_file)"
      ],
      "id": "R35x-wIkdY8P"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5d99df2-ab0b-4e2e-b775-201971e35d10",
        "outputId": "47a9cdce-0207-4333-b810-1579a22660c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys: dict_keys(['data_50sec', 'spec_10min', 'subject_ID', 'votes'])\n",
            "data_50sec: Shape=(20, 10000)\n",
            "votes: [ 0.  0.  1. 10.  0.  0.]\n",
            "subject_ID: sid1056\n",
            "spec_10min[0]: ['LL', numpy.ndarray with shape=(100, 300)]\n",
            "spec_10min[1]: ['RL', numpy.ndarray with shape=(100, 300)]\n",
            "spec_10min[2]: ['LP', numpy.ndarray with shape=(100, 300)]\n",
            "spec_10min[3]: ['RP', numpy.ndarray with shape=(100, 300)]\n"
          ]
        }
      ],
      "source": [
        "# Examine data (you can learn more about this data on the competition's Eventbrite webpage)\n",
        "import mat73\n",
        "_sample = mat73.loadmat(path)\n",
        "print(f\"Keys: {_sample.keys()}\")\n",
        "print(f\"data_50sec: Shape={_sample['data_50sec'].shape}\")\n",
        "print(f\"votes: {_sample['votes']}\")\n",
        "print(f\"subject_ID: {_sample['subject_ID']}\")\n",
        "for _ in range(4):\n",
        "    print(f\"spec_10min[{_}]: ['{_sample['spec_10min'][_][0]}', numpy.ndarray with shape={_sample['spec_10min'][_][1].shape}]\")"
      ],
      "id": "d5d99df2-ab0b-4e2e-b775-201971e35d10"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9bbcc674-f5f9-45aa-ac09-48f40bd0d26e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4548f76-81a5-48d0-f041-7d4e7f821914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.persist_to_disk does not exist. Creating it for persist_to_disk\n",
            "/root/.persist_to_disk/cache does not exist. Creating it for persist_to_disk\n",
            "/root/.persist_to_disk/cache/content-1 does not exist. Creating it for persist_to_disk\n",
            "/root/.persist_to_disk/cache/content-1/data_utils does not exist. Creating it for persist_to_disk\n",
            "/root/.persist_to_disk/cache/content-1/data_utils/_read_and_transform_x does not exist. Creating it for persist_to_disk\n",
            "/root/.persist_to_disk/cache/content-1/data_utils/_read_labels does not exist. Creating it for persist_to_disk\n",
            "Reading labels...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:07<00:00, 28.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting Patients...\n",
            "Reading signals...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 136/136 [00:03<00:00, 44.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading labels...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:00<00:00, 4815.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting Patients...\n",
            "Reading signals...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:01<00:00, 45.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of train samples=136\n",
            "number of train subjects=59\n",
            "number of valid samples=64\n",
            "number of valid subjects=26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# We will just load the data using data_utils. Note that this step may take several minutes.\n",
        "# In the following, the test set is not loaded yet - we will load it in batches later \n",
        "\n",
        "#Load training data for one batch of data\n",
        "#NOTE: change batchdir to batch02, batch03, or batch04 to load more training data.\n",
        "batchdir=\"batch01\"\n",
        "\n",
        "import data_utils\n",
        "reload(data_utils)\n",
        "train_data = data_utils.ColumbiaData('train', split_ratio=[0.7, 0.3, 0.], data_dir=batchdir, debug=DEBUG)\n",
        "valid_data = data_utils.ColumbiaData('val', split_ratio=[0.7, 0.3, 0.], data_dir=batchdir, debug=DEBUG)\n",
        "#test_data = data_utils.ColumbiaData('test', data_dir=trainDir, debug=DEBUG)\n",
        "for _split, _data in zip(['train', 'valid'], [train_data, valid_data]):\n",
        "    print(f\"number of {_split} samples={len(_data)}\")\n",
        "    print(f\"number of {_split} subjects={_data._infos['subject_ID'].nunique()}\")\n"
      ],
      "id": "9bbcc674-f5f9-45aa-ac09-48f40bd0d26e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24d928ef-0a6b-4831-ab8b-c5ec9b55e9b8"
      },
      "source": [
        "## Train the Model"
      ],
      "id": "24d928ef-0a6b-4831-ab8b-c5ec9b55e9b8"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9280d8b5-65a5-4d4d-b24c-ae8201086246",
        "outputId": "5d2c4aba-1624-4b61-fd1d-ef4f5c866e52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNNEncoder2D_IIIC(\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv2d(32, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ELU(alpha=1.0, inplace=True)\n",
              "  )\n",
              "  (conv2): ResBlock(\n",
              "    (conv1): Conv2d(48, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ELU(alpha=1.0, inplace=True)\n",
              "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (downsample): Sequential(\n",
              "      (0): Conv2d(48, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (conv3): ResBlock(\n",
              "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ELU(alpha=1.0, inplace=True)\n",
              "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (downsample): Sequential(\n",
              "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (conv4): ResBlock(\n",
              "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ELU(alpha=1.0, inplace=True)\n",
              "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (downsample): Sequential(\n",
              "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (sup): Sequential(\n",
              "    (0): Linear(in_features=1024, out_features=96, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=96, out_features=6, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# We use a CNN on spectrogram - the model code was loaded with the helper files earlier\n",
        "from model import CNNEncoder2D_IIIC\n",
        "\n",
        "model = CNNEncoder2D_IIIC(nclass=6, num_channels=16)\n",
        "model"
      ],
      "id": "9280d8b5-65a5-4d4d-b24c-ae8201086246"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "78c82f92-4e86-4b74-b214-f009f23675e6"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import tqdm\n",
        "import numpy as np\n",
        "from utils import concat_list_of_dict\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model(model, valid_data, batch_size=128, device='cpu', num_workers=0):\n",
        "    model.eval()\n",
        "    valid_loader = DataLoader(dataset=valid_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    preds = []\n",
        "    for batch in tqdm.tqdm(valid_loader):\n",
        "        logits = model(batch.pop('data').to(device))\n",
        "        batch['logits'] = logits.cpu()\n",
        "        preds.append(batch)\n",
        "    return concat_list_of_dict(preds, ['logits', 'target', 'index'])\n",
        "\n",
        "def train(model, train_data, valid_data=None, batch_size=128, epochs=30, debug=False, device='cuda:0', num_workers=4):\n",
        "    if debug:\n",
        "        device, num_workers, epochs = 'cpu', 0, 3\n",
        "    model = model.to(device)\n",
        "\n",
        "    ckpt_path = os.path.join(CHECKPOINT_DIR, 'best_ckpt.pth')\n",
        "    curr_val_loss = best_val_loss = np.inf\n",
        "    train_loss = []\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "    for curr_epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in tqdm.tqdm(train_loader, desc=f'Training Epoch={curr_epoch}'):\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(batch['data'].to(device))\n",
        "            loss = criterion(logits, batch['target'].to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss.append(loss.item())\n",
        "        if valid_data is not None:\n",
        "            eval_results = eval_model(model, valid_data, batch_size=batch_size, device=device, num_workers=num_workers)\n",
        "            curr_val_loss = criterion(torch.tensor(eval_results['logits']),\n",
        "                                      torch.tensor(eval_results['target'])).item()\n",
        "            if curr_val_loss < best_val_loss:\n",
        "                best_val_loss = curr_val_loss\n",
        "                torch.save(model.state_dict(), ckpt_path)\n",
        "        print(f\"Train Loss={np.mean(train_loss):.3f}, Val Loss={curr_val_loss:.3f}, Best Val Loss={best_val_loss:.3f}\")\n",
        "    model.load_state_dict(torch.load(ckpt_path))\n",
        "    return model"
      ],
      "id": "78c82f92-4e86-4b74-b214-f009f23675e6"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc1cf9cb-81b7-423e-aaf2-1d858b381916",
        "outputId": "cc9587ea-9484-430e-be09-16995a0c08a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch=0: 100%|██████████| 2/2 [00:04<00:00,  2.42s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss=1.886, Val Loss=1.909, Best Val Loss=1.909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch=1: 100%|██████████| 2/2 [00:03<00:00,  1.98s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss=1.862, Val Loss=1.692, Best Val Loss=1.692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch=2: 100%|██████████| 2/2 [00:03<00:00,  1.80s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss=1.820, Val Loss=1.564, Best Val Loss=1.564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "trained_model = train(model, train_data, valid_data, debug=DEBUG)"
      ],
      "id": "bc1cf9cb-81b7-423e-aaf2-1d858b381916"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59e37765-4002-4951-a40c-b659f47f5c78"
      },
      "source": [
        "## Generate test predictions to submit to competition leaderboard"
      ],
      "id": "59e37765-4002-4951-a40c-b659f47f5c78"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9tgxKV4URYJ",
        "outputId": "cd481564-5d1e-4c50-8444-df5a7a8427fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 78/78 [02:08<00:00,  1.65s/it]\n",
            "100%|██████████| 79/79 [01:52<00:00,  1.42s/it]\n",
            "100%|██████████| 79/79 [01:48<00:00,  1.37s/it]\n",
            "100%|██████████| 45/45 [00:56<00:00,  1.26s/it]\n"
          ]
        }
      ],
      "source": [
        "#Generate predicted labels from entire test dataset (in four batches)\n",
        "# Test data stored in pickle files.  NOTE: you may need to adjust filepaths to...\n",
        "#...load test data files correctly below.\n",
        "\n",
        "# Note that the file paths below may need to be adjusted if the test data pickle files were saved \n",
        "# anywhere other than the default location \n",
        "def get_prediction_labels(trainedmodel, testdata):\n",
        "      test_preds = eval_model(trainedmodel, testdata)\n",
        "      import pandas as pd\n",
        "      test_pred_df = pd.DataFrame(test_preds['logits'], columns=[f'pred_{_}' for _ in range(6)])\n",
        "      CLASSES = ['Other', 'Seizure', 'LPD', 'GPD', 'LRDA', 'GRDA']\n",
        "      test_pred_df['pred'] = np.argmax(test_preds['logits'], axis=1)\n",
        "\n",
        "      predicted_labels = []\n",
        "\n",
        "      for i in test_pred_df['pred']:\n",
        "          label=CLASSES[i]\n",
        "          predicted_labels.append(label)\n",
        "\n",
        "      test_pred_df['predicted_labels']=predicted_labels\n",
        "      test_pred_df['index'] = test_preds['index']\n",
        "      test_pred_df['target'] = test_preds['target']\n",
        "      return list(test_pred_df['predicted_labels'])\n",
        "\n",
        "import pickle\n",
        "file = open('drive/MyDrive/Harvard Hospital/batch1.pickle', 'rb')\n",
        "b1data = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "predicted_labels_batch1=get_prediction_labels(trained_model,b1data)\n",
        "\n",
        "del(b1data)\n",
        "\n",
        "file = open('drive/MyDrive/Harvard Hospital/batch2.pickle', 'rb')\n",
        "b2data = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "predicted_labels_batch2=get_prediction_labels(trained_model,b2data)\n",
        "\n",
        "del(b2data)\n",
        "\n",
        "file = open('drive/MyDrive/Harvard Hospital/batch3.pickle', 'rb')\n",
        "b3data = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "predicted_labels_batch3=get_prediction_labels(trained_model,b3data)\n",
        "\n",
        "del(b3data)\n",
        "\n",
        "file = open('drive/MyDrive/Harvard Hospital/batch4.pickle', 'rb')\n",
        "b4data = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "predicted_labels_batch4=get_prediction_labels(trained_model,b4data)\n",
        "\n",
        "del(b4data)"
      ],
      "id": "h9tgxKV4URYJ"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zX0OJGQcVmUV"
      },
      "outputs": [],
      "source": [
        "#Combine predicted labels to submit to leaderboard \n",
        "predicted_labels=predicted_labels_batch1+predicted_labels_batch2+predicted_labels_batch3+predicted_labels_batch4"
      ],
      "id": "zX0OJGQcVmUV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Op7FpTUchCk3"
      },
      "outputs": [],
      "source": [
        "#install aimodelshare library to make competition leaderboard submissions\n",
        "! pip install aimodelshare --upgrade"
      ],
      "id": "Op7FpTUchCk3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hks1trIE372n"
      },
      "source": [
        "To submit models to the competition leaderboard, you will need credentials for modelshare.ai \n",
        "\n",
        "[Create a free account here](https://www.modelshare.ai/)"
      ],
      "id": "Hks1trIE372n"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "lXuLPcnUgwya"
      },
      "outputs": [],
      "source": [
        "from aimodelshare import ModelPlayground\n",
        "from aimodelshare.aws import set_credentials\n",
        "\n",
        "playground_id=\"https://negjs28m2m.execute-api.us-east-2.amazonaws.com/prod/m\"\n",
        "myplayground = ModelPlayground(playground_url = playground_id, task_type=\"classification\")\n",
        "set_credentials(apiurl=playground_id)"
      ],
      "id": "lXuLPcnUgwya"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WtGfOd4hr8X"
      },
      "outputs": [],
      "source": [
        "# Submit Model predictions to Competition Leaderboard\n",
        "# To win the competition, be sure to share the code you used to preprocess data and generate your model...\n",
        "#...on the code tab of the competition webpage here: https://www.modelshare.ai/detail/model:3691 \n",
        "\n",
        "myplayground.submit_model(model=None,\n",
        "                          preprocessor=None,\n",
        "                          prediction_submission=predicted_labels,submission_type=\"competition\")"
      ],
      "id": "4WtGfOd4hr8X"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "CyMZUwPzjC1r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "815a233d-a9ba-4417-daa3-edd844d452c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fd44d0535e0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_1aab1_row0_col0 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  background: linear-gradient(90deg, #f5f8d6 17.1%, transparent 17.1%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_1aab1_row0_col1 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  background: linear-gradient(90deg, #c778c8 15.7%, transparent 15.7%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_1aab1_row0_col2 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  background: linear-gradient(90deg, #ff4971 17.2%, transparent 17.2%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_1aab1_row0_col3 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  background: linear-gradient(90deg, #aadbaa 16.6%, transparent 16.6%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_1aab1_row0_col4, #T_1aab1_row0_col5, #T_1aab1_row0_col6, #T_1aab1_row0_col7, #T_1aab1_row1_col4, #T_1aab1_row1_col5, #T_1aab1_row1_col6, #T_1aab1_row1_col7, #T_1aab1_row2_col4, #T_1aab1_row2_col5, #T_1aab1_row2_col6, #T_1aab1_row2_col7, #T_1aab1_row3_col4, #T_1aab1_row3_col5, #T_1aab1_row3_col6, #T_1aab1_row3_col7, #T_1aab1_row4_col4, #T_1aab1_row4_col5, #T_1aab1_row4_col6, #T_1aab1_row4_col7 {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_1aab1_row1_col0, #T_1aab1_row2_col0, #T_1aab1_row3_col0 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  background: linear-gradient(90deg, #f5f8d6 26.5%, transparent 26.5%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_1aab1_row1_col1, #T_1aab1_row2_col1, #T_1aab1_row3_col1 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  background: linear-gradient(90deg, #c778c8 10.7%, transparent 10.7%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_1aab1_row1_col2, #T_1aab1_row2_col2, #T_1aab1_row3_col2 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  background: linear-gradient(90deg, #ff4971 8.7%, transparent 8.7%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_1aab1_row1_col3, #T_1aab1_row2_col3, #T_1aab1_row3_col3 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  background: linear-gradient(90deg, #aadbaa 17.1%, transparent 17.1%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_1aab1_row4_col0 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  background: linear-gradient(90deg, #f5f8d6 22.4%, transparent 22.4%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_1aab1_row4_col1 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  background: linear-gradient(90deg, #c778c8 12.7%, transparent 12.7%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_1aab1_row4_col2 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  background: linear-gradient(90deg, #ff4971 13.9%, transparent 13.9%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_1aab1_row4_col3 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  background: linear-gradient(90deg, #aadbaa 16.0%, transparent 16.0%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_1aab1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_1aab1_level0_col0\" class=\"col_heading level0 col0\" >accuracy</th>\n",
              "      <th id=\"T_1aab1_level0_col1\" class=\"col_heading level0 col1\" >f1_score</th>\n",
              "      <th id=\"T_1aab1_level0_col2\" class=\"col_heading level0 col2\" >precision</th>\n",
              "      <th id=\"T_1aab1_level0_col3\" class=\"col_heading level0 col3\" >recall</th>\n",
              "      <th id=\"T_1aab1_level0_col4\" class=\"col_heading level0 col4\" >ml_framework</th>\n",
              "      <th id=\"T_1aab1_level0_col5\" class=\"col_heading level0 col5\" >model_type</th>\n",
              "      <th id=\"T_1aab1_level0_col6\" class=\"col_heading level0 col6\" >username</th>\n",
              "      <th id=\"T_1aab1_level0_col7\" class=\"col_heading level0 col7\" >version</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_1aab1_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_1aab1_row0_col0\" class=\"data row0 col0\" >17.06%</td>\n",
              "      <td id=\"T_1aab1_row0_col1\" class=\"data row0 col1\" >15.68%</td>\n",
              "      <td id=\"T_1aab1_row0_col2\" class=\"data row0 col2\" >17.17%</td>\n",
              "      <td id=\"T_1aab1_row0_col3\" class=\"data row0 col3\" >16.65%</td>\n",
              "      <td id=\"T_1aab1_row0_col4\" class=\"data row0 col4\" >unknown</td>\n",
              "      <td id=\"T_1aab1_row0_col5\" class=\"data row0 col5\" >unknown</td>\n",
              "      <td id=\"T_1aab1_row0_col6\" class=\"data row0 col6\" >AIModelShare</td>\n",
              "      <td id=\"T_1aab1_row0_col7\" class=\"data row0 col7\" >1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_1aab1_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_1aab1_row1_col0\" class=\"data row1 col0\" >26.51%</td>\n",
              "      <td id=\"T_1aab1_row1_col1\" class=\"data row1 col1\" >10.73%</td>\n",
              "      <td id=\"T_1aab1_row1_col2\" class=\"data row1 col2\" >8.68%</td>\n",
              "      <td id=\"T_1aab1_row1_col3\" class=\"data row1 col3\" >17.07%</td>\n",
              "      <td id=\"T_1aab1_row1_col4\" class=\"data row1 col4\" >unknown</td>\n",
              "      <td id=\"T_1aab1_row1_col5\" class=\"data row1 col5\" >unknown</td>\n",
              "      <td id=\"T_1aab1_row1_col6\" class=\"data row1 col6\" >newusertest</td>\n",
              "      <td id=\"T_1aab1_row1_col7\" class=\"data row1 col7\" >3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_1aab1_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_1aab1_row2_col0\" class=\"data row2 col0\" >26.51%</td>\n",
              "      <td id=\"T_1aab1_row2_col1\" class=\"data row2 col1\" >10.73%</td>\n",
              "      <td id=\"T_1aab1_row2_col2\" class=\"data row2 col2\" >8.68%</td>\n",
              "      <td id=\"T_1aab1_row2_col3\" class=\"data row2 col3\" >17.07%</td>\n",
              "      <td id=\"T_1aab1_row2_col4\" class=\"data row2 col4\" >unknown</td>\n",
              "      <td id=\"T_1aab1_row2_col5\" class=\"data row2 col5\" >unknown</td>\n",
              "      <td id=\"T_1aab1_row2_col6\" class=\"data row2 col6\" >newusertest</td>\n",
              "      <td id=\"T_1aab1_row2_col7\" class=\"data row2 col7\" >4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_1aab1_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_1aab1_row3_col0\" class=\"data row3 col0\" >26.51%</td>\n",
              "      <td id=\"T_1aab1_row3_col1\" class=\"data row3 col1\" >10.73%</td>\n",
              "      <td id=\"T_1aab1_row3_col2\" class=\"data row3 col2\" >8.68%</td>\n",
              "      <td id=\"T_1aab1_row3_col3\" class=\"data row3 col3\" >17.07%</td>\n",
              "      <td id=\"T_1aab1_row3_col4\" class=\"data row3 col4\" >unknown</td>\n",
              "      <td id=\"T_1aab1_row3_col5\" class=\"data row3 col5\" >unknown</td>\n",
              "      <td id=\"T_1aab1_row3_col6\" class=\"data row3 col6\" >newusertest</td>\n",
              "      <td id=\"T_1aab1_row3_col7\" class=\"data row3 col7\" >5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_1aab1_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_1aab1_row4_col0\" class=\"data row4 col0\" >22.40%</td>\n",
              "      <td id=\"T_1aab1_row4_col1\" class=\"data row4 col1\" >12.65%</td>\n",
              "      <td id=\"T_1aab1_row4_col2\" class=\"data row4 col2\" >13.87%</td>\n",
              "      <td id=\"T_1aab1_row4_col3\" class=\"data row4 col3\" >15.99%</td>\n",
              "      <td id=\"T_1aab1_row4_col4\" class=\"data row4 col4\" >unknown</td>\n",
              "      <td id=\"T_1aab1_row4_col5\" class=\"data row4 col5\" >unknown</td>\n",
              "      <td id=\"T_1aab1_row4_col6\" class=\"data row4 col6\" >gstreett</td>\n",
              "      <td id=\"T_1aab1_row4_col7\" class=\"data row4 col7\" >2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# Check competition leaderboard\n",
        "data = myplayground.get_leaderboard(submission_type=\"competition\")\n",
        "myplayground.stylize_leaderboard(data)"
      ],
      "id": "CyMZUwPzjC1r"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}